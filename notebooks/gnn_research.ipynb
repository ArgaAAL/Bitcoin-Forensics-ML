{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db7901b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ADVANCED GNN RANSOMWARE DETECTION WITH LEAKAGE FIXES\n",
      "======================================================================\n",
      "[RESUME] Loading data from checkpoint...\n",
      "[SUCCESS] Resumed with 1268260 wallet records and 2868964 edges\n",
      "[RESUME] Loading feature engineering from checkpoint...\n",
      "[SUCCESS] Resumed with 68 columns\n",
      "[RESUME] Loading cleaned data from checkpoint...\n",
      "[SUCCESS] Resumed with 367472 cleaned addresses\n",
      "[RESUME] Loading advanced graph from checkpoint...\n",
      "[SUCCESS] Resumed with graph: 265354 nodes, 2236444 edges\n",
      "[CRITICAL FIX] Creating proper train/test split...\n",
      "Train nodes: 212283\n",
      "Test nodes: 53071\n",
      "[ADVANCED] Applying SMOTE to training data...\n",
      "Original: [200870  11413]\n",
      "SMOTE: [200870 200870]\n",
      "[INFO] Using class weights due to graph structure complexity\n",
      "[ADVANCED] Initializing advanced GNN model...\n",
      "[INFO] Using device: cuda\n",
      "[ADVANCED] Setting up Focal Loss and optimizer...\n",
      "[INFO] Creating NeighborLoader...\n",
      "[TRAINING] Starting advanced training...\n",
      "‚ùå Error occurred: CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.45 GiB is allocated by PyTorch, and 3.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "üí° Check if all required data files are available and paths are correct.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Temp\\ipykernel_6624\\914404923.py\", line 844, in <module>\n",
      "    results = main_advanced_gnn_training()\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Temp\\ipykernel_6624\\914404923.py\", line 595, in main_advanced_gnn_training\n",
      "    results = evaluate_without_leakage(model, data, test_idx, device, threshold=0.5)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Temp\\ipykernel_6624\\914404923.py\", line 100, in evaluate_without_leakage\n",
      "    out = model(subgraph_x, subgraph_edges_remapped)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Temp\\ipykernel_6624\\914404923.py\", line 252, in forward\n",
      "    h_gat = F.elu(gat_layer(current_x, edge_index))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py\", line 366, in forward\n",
      "    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CATURW~1\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gat_conv_GATConv_propagate_m9_zrztv.py\", line 176, in propagate\n",
      "    out = self.message(\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py\", line 414, in message\n",
      "    return alpha.unsqueeze(-1) * x_j\n",
      "           ~~~~~~~~~~~~~~~~~~~~^~~~~\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.45 GiB is allocated by PyTorch, and 3.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, GraphSAGE, TransformerConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mount Google Drive (if using Colab)\n",
    "try:\n",
    "    CHECKPOINT_DIR = 'Ransomware_Checkpoints_v2'\n",
    "except:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL FIX 1: FOCAL LOSS FOR EXTREME IMBALANCE\n",
    "# ============================================================================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL FIX 2: LEAKAGE-FREE EVALUATION\n",
    "# ============================================================================\n",
    "def evaluate_without_leakage(model, data, test_indices, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set WITHOUT data leakage\n",
    "    Uses only test nodes without neighbor sampling\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create a mapping from original indices to test subgraph indices\n",
    "        test_nodes = torch.tensor(test_indices, device=device)\n",
    "        \n",
    "        # Get the subgraph containing test nodes and their direct neighbors\n",
    "        # This is necessary because GNN needs neighbor information\n",
    "        all_edges = data.edge_index\n",
    "        \n",
    "        # Find all neighbors of test nodes\n",
    "        test_neighbors = set()\n",
    "        for node in test_indices:\n",
    "            # Find edges where this node appears\n",
    "            neighbors = all_edges[1][all_edges[0] == node].cpu().numpy()\n",
    "            test_neighbors.update(neighbors)\n",
    "            neighbors = all_edges[0][all_edges[1] == node].cpu().numpy()\n",
    "            test_neighbors.update(neighbors)\n",
    "        \n",
    "        # Combine test nodes with their neighbors\n",
    "        subgraph_nodes = list(set(test_indices) | test_neighbors)\n",
    "        subgraph_nodes = torch.tensor(subgraph_nodes, device=device)\n",
    "        \n",
    "        # Create node mapping\n",
    "        node_mapping = {old_idx.item(): new_idx for new_idx, old_idx in enumerate(subgraph_nodes)}\n",
    "        \n",
    "        # Extract subgraph\n",
    "        subgraph_x = data.x[subgraph_nodes]\n",
    "        \n",
    "        # Filter edges to only include those within the subgraph\n",
    "        mask = torch.isin(all_edges[0], subgraph_nodes) & torch.isin(all_edges[1], subgraph_nodes)\n",
    "        subgraph_edges = all_edges[:, mask]\n",
    "        \n",
    "        # Remap edge indices\n",
    "        subgraph_edges_remapped = torch.zeros_like(subgraph_edges)\n",
    "        for i in range(subgraph_edges.shape[1]):\n",
    "            subgraph_edges_remapped[0, i] = node_mapping[subgraph_edges[0, i].item()]\n",
    "            subgraph_edges_remapped[1, i] = node_mapping[subgraph_edges[1, i].item()]\n",
    "        \n",
    "        # Forward pass on subgraph\n",
    "        out = model(subgraph_x, subgraph_edges_remapped)\n",
    "        \n",
    "        # Extract predictions only for original test nodes\n",
    "        test_node_indices_in_subgraph = [node_mapping[idx] for idx in test_indices if idx in node_mapping]\n",
    "        test_out = out[test_node_indices_in_subgraph]\n",
    "        \n",
    "        # Get predictions and probabilities\n",
    "        pred_proba = F.softmax(test_out, dim=1)[:, 1]\n",
    "        pred_binary = (pred_proba >= threshold).long()\n",
    "        pred_argmax = test_out.argmax(dim=1)\n",
    "        \n",
    "        # Get true labels for test nodes\n",
    "        true_labels = data.y[test_indices]\n",
    "        \n",
    "    return {\n",
    "        'predictions': pred_binary.cpu().numpy(),\n",
    "        'predictions_argmax': pred_argmax.cpu().numpy(),\n",
    "        'probabilities': pred_proba.cpu().numpy(),\n",
    "        'true_labels': true_labels.cpu().numpy()\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ADVANCED FEATURE ENGINEERING: GRAPH CENTRALITY MEASURES\n",
    "# ============================================================================\n",
    "def compute_graph_centrality_features(edge_index, num_nodes):\n",
    "    \"\"\"Compute centrality measures for graph nodes\"\"\"\n",
    "    print(\"Computing graph centrality features...\")\n",
    "    \n",
    "    # Convert to NetworkX graph\n",
    "    edge_list = edge_index.t().cpu().numpy()\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    G.add_edges_from(edge_list)\n",
    "    \n",
    "    # Compute centrality measures\n",
    "    print(\"  - Computing PageRank...\")\n",
    "    pagerank = nx.pagerank(G, max_iter=50)\n",
    "    \n",
    "    print(\"  - Computing Betweenness Centrality...\")\n",
    "    betweenness = nx.betweenness_centrality(G, k=min(1000, num_nodes))\n",
    "    \n",
    "    # print(\"  - Computing Closeness Centrality...\")\n",
    "    # closeness = nx.closeness_centrality(G)\n",
    "    \n",
    "    print(\"  - Computing Degree Centrality...\")\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    pagerank_array = np.array([pagerank.get(i, 0) for i in range(num_nodes)])\n",
    "    betweenness_array = np.array([betweenness.get(i, 0) for i in range(num_nodes)])\n",
    "    # closeness_array = np.array([closeness.get(i, 0) for i in range(num_nodes)])\n",
    "    degree_array = np.array([degree_centrality.get(i, 0) for i in range(num_nodes)])\n",
    "    \n",
    "    return {\n",
    "        'pagerank': pagerank_array,\n",
    "        'betweenness': betweenness_array,\n",
    "        # 'closeness': closeness_array,\n",
    "        'degree_centrality': degree_array\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ADVANCED ARCHITECTURE: MULTI-LAYER GNN WITH ATTENTION FUSION\n",
    "# ============================================================================\n",
    "class AdvancedGNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super(AdvancedGNN, self).__init__()\n",
    "        \n",
    "        # --- FIX: Define the input dimension for the second and third layers ---\n",
    "        # This is the dimension after concatenating the first GAT and GCN outputs.\n",
    "        layer2_input_dim = (hidden_dim * 4) + hidden_dim  # 512 + 128 = 640\n",
    "        \n",
    "        # --- FIX: Define the input dimension for the final layer ---\n",
    "        # This is the dimension after concatenating the second GAT and GCN outputs.\n",
    "        layer3_input_dim = (hidden_dim * 4) + hidden_dim  # 512 + 128 = 640\n",
    "\n",
    "        # Multiple GNN Types\n",
    "        self.gat_layers = nn.ModuleList([\n",
    "            GATConv(num_features, hidden_dim, heads=4, dropout=dropout),\n",
    "            # --- FIX: Correct the input dimension ---\n",
    "            GATConv(layer2_input_dim, hidden_dim, heads=4, dropout=dropout),\n",
    "            # --- FIX: Correct the input dimension ---\n",
    "            GATConv(layer3_input_dim, hidden_dim // 2, heads=2, dropout=dropout)\n",
    "        ])\n",
    "        \n",
    "        self.gcn_layers = nn.ModuleList([\n",
    "            GCNConv(num_features, hidden_dim),\n",
    "            # --- FIX: Correct the input dimension ---\n",
    "            GCNConv(layer2_input_dim, hidden_dim),\n",
    "            # --- FIX: Correct the input dimension ---\n",
    "            GCNConv(layer3_input_dim, hidden_dim // 2)\n",
    "        ])\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn_layers = nn.ModuleList([\n",
    "            nn.BatchNorm1d(hidden_dim * 4 + hidden_dim),  # GAT(512) + GCN(128) = 640\n",
    "            nn.BatchNorm1d(hidden_dim * 4 + hidden_dim),  # GAT(512) + GCN(128) = 640\n",
    "            nn.BatchNorm1d((hidden_dim // 2 * 2) + (hidden_dim // 2)) # GAT(128) + GCN(64) = 192\n",
    "        ])\n",
    "        \n",
    "        # Residual connections\n",
    "        self.residual_projections = nn.ModuleList([\n",
    "            nn.Linear(num_features, layer2_input_dim),\n",
    "            # --- FIX: Correct the input dimension for the projection ---\n",
    "            nn.Linear(layer2_input_dim, layer3_input_dim),\n",
    "            # --- FIX: Correct the input dimension for the projection ---\n",
    "            nn.Linear(layer3_input_dim, (hidden_dim // 2 * 2) + (hidden_dim // 2))\n",
    "        ])\n",
    "        \n",
    "        # Feature-only path (XGBoost mimic)\n",
    "        self.feature_path = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Combined classifier\n",
    "        # --- FIX: Adjust combined_dim based on the final layer's output ---\n",
    "        final_graph_dim = (hidden_dim // 2 * 2) + (hidden_dim // 2) # 192\n",
    "        feature_only_dim = hidden_dim // 2 # 64\n",
    "        combined_dim = final_graph_dim + feature_only_dim # 192 + 64 = 256\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(hidden_dim // 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Store original features for the feature-only path\n",
    "        original_x = x\n",
    "        current_x = x\n",
    "        \n",
    "        for i, (gat_layer, gcn_layer) in enumerate(zip(self.gat_layers, self.gcn_layers)):\n",
    "            # --- FIX: Store the input of the current block for the residual connection ---\n",
    "            input_for_residual = current_x\n",
    "            \n",
    "            # GAT path\n",
    "            h_gat = F.elu(gat_layer(current_x, edge_index))\n",
    "            \n",
    "            # GCN path\n",
    "            h_gcn = F.elu(gcn_layer(current_x, edge_index))\n",
    "            \n",
    "            # Concatenate different GNN outputs\n",
    "            h_combined = torch.cat([h_gat, h_gcn], dim=1)\n",
    "            \n",
    "            # Residual connection\n",
    "            residual = self.residual_projections[i](input_for_residual)\n",
    "            \n",
    "            # Add residual connection\n",
    "            h_combined = h_combined + residual\n",
    "\n",
    "            # Batch normalization\n",
    "            h_combined = self.bn_layers[i](h_combined)\n",
    "            \n",
    "            # Dropout\n",
    "            current_x = F.dropout(h_combined, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Graph features are the final output of the GNN blocks\n",
    "        graph_features = current_x\n",
    "        \n",
    "        # Feature-only path\n",
    "        feature_only = self.feature_path(original_x)\n",
    "        \n",
    "        # Combine all paths\n",
    "        combined = torch.cat([graph_features, feature_only], dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.classifier(combined)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "# ============================================================================\n",
    "# CHECKPOINT SYSTEM (KEEPING YOUR EXISTING SYSTEM)\n",
    "# ============================================================================\n",
    "def save_checkpoint(checkpoint_name, data, message=\"Checkpoint saved\"):\n",
    "    \"\"\"Save checkpoint data\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"‚úÖ {message}: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_name):\n",
    "    \"\"\"Load checkpoint data\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def checkpoint_exists(checkpoint_name):\n",
    "    \"\"\"Check if checkpoint exists\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    return os.path.exists(checkpoint_path)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN INTEGRATION FUNCTION\n",
    "# ============================================================================\n",
    "def main_advanced_gnn_training():\n",
    "    \"\"\"Complete advanced GNN training with your existing structure\"\"\"\n",
    "    \n",
    "    print(\"üöÄ ADVANCED GNN RANSOMWARE DETECTION WITH LEAKAGE FIXES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # STEP 1: Data Loading (keeping your existing logic)\n",
    "    if checkpoint_exists('data_loaded'):\n",
    "        print(\"[RESUME] Loading data from checkpoint...\")\n",
    "        checkpoint_data = load_checkpoint('data_loaded')\n",
    "        wallets_df = checkpoint_data['wallets_df']\n",
    "        edges_df = checkpoint_data['edges_df']\n",
    "        print(f\"[SUCCESS] Resumed with {len(wallets_df)} wallet records and {len(edges_df)} edges\")\n",
    "    else:\n",
    "        print(\"[INFO] Loading wallet features and classes...\")\n",
    "        wallets_df = pd.read_csv('Elliptic++ Dataset/wallets_features_classes_combined.csv')\n",
    "        print(f\"[SUCCESS] Loaded {len(wallets_df)} wallet records\")\n",
    "\n",
    "        print(\"[INFO] Loading address-to-address edges...\")\n",
    "        edges_df = pd.read_csv('Elliptic++ Dataset/AddrAddr_edgelist.csv')\n",
    "        print(f\"[SUCCESS] Loaded {len(edges_df)} edges\")\n",
    "\n",
    "        save_checkpoint('data_loaded', {\n",
    "            'wallets_df': wallets_df,\n",
    "            'edges_df': edges_df\n",
    "        }, \"Data loading checkpoint saved\")\n",
    "\n",
    "    # STEP 2: Feature Engineering (using your enhanced features)\n",
    "    def create_enhanced_pattern_features(df):\n",
    "        \"\"\"Create the EXACT same enhanced features as XGBoost model\"\"\"\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "\n",
    "        # Your existing feature engineering\n",
    "        df['partner_transaction_ratio'] = (\n",
    "            df.get('transacted_w_address_total', 0) /\n",
    "            (df.get('total_txs', 1) + 1e-8)\n",
    "        )\n",
    "        df['activity_density'] = (\n",
    "            df.get('total_txs', 0) /\n",
    "            (df.get('lifetime_in_blocks', 1) + 1e-8)\n",
    "        )\n",
    "        df['transaction_size_variance'] = (\n",
    "            df.get('btc_transacted_max', 0) - df.get('btc_transacted_min', 0)\n",
    "        ) / (df.get('btc_transacted_mean', 1) + 1e-8)\n",
    "        df['flow_imbalance'] = (\n",
    "            (df.get('btc_sent_total', 0) - df.get('btc_received_total', 0)) /\n",
    "            (df.get('btc_transacted_total', 1) + 1e-8)\n",
    "        )\n",
    "        df['temporal_spread'] = (\n",
    "            df.get('last_block_appeared_in', 0) - df.get('first_block_appeared_in', 0)\n",
    "        ) / (df.get('num_timesteps_appeared_in', 1) + 1e-8)\n",
    "        df['fee_percentile'] = (\n",
    "            df.get('fees_total', 0) /\n",
    "            (df.get('btc_transacted_total', 1) + 1e-8)\n",
    "        )\n",
    "        df['interaction_intensity'] = (\n",
    "            df.get('num_addr_transacted_multiple', 0) /\n",
    "            (df.get('transacted_w_address_total', 1) + 1e-8)\n",
    "        )\n",
    "        df['value_per_transaction'] = (\n",
    "            df.get('btc_transacted_total', 0) /\n",
    "            (df.get('total_txs', 1) + 1e-8)\n",
    "        )\n",
    "        df['burst_activity'] = (\n",
    "            df.get('total_txs', 0) * df.get('activity_density', 0)\n",
    "        )\n",
    "        df['mixing_intensity'] = (\n",
    "            df.get('partner_transaction_ratio', 0) * df.get('interaction_intensity', 0)\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    if checkpoint_exists('features_engineered'):\n",
    "        print(\"[RESUME] Loading feature engineering from checkpoint...\")\n",
    "        checkpoint_data = load_checkpoint('features_engineered')\n",
    "        wallets_df = checkpoint_data['wallets_df']\n",
    "        print(f\"[SUCCESS] Resumed with {len(wallets_df.columns)} columns\")\n",
    "    else:\n",
    "        print(\"[INFO] Creating enhanced features...\")\n",
    "        wallets_df = create_enhanced_pattern_features(wallets_df)\n",
    "        print(f\"[SUCCESS] Enhanced features created. Now have {len(wallets_df.columns)} columns\")\n",
    "        save_checkpoint('features_engineered', {'wallets_df': wallets_df}, \"Feature engineering checkpoint saved\")\n",
    "\n",
    "    # STEP 3: Data Cleaning (keeping your existing logic)\n",
    "    if checkpoint_exists('data_cleaned'):\n",
    "        print(\"[RESUME] Loading cleaned data from checkpoint...\")\n",
    "        checkpoint_data = load_checkpoint('data_cleaned')\n",
    "        wallets_clean = checkpoint_data['wallets_clean']\n",
    "        class_weight_dict = checkpoint_data['class_weight_dict']\n",
    "        print(f\"[SUCCESS] Resumed with {len(wallets_clean)} cleaned addresses\")\n",
    "    else:\n",
    "        print(\"[INFO] Cleaning data...\")\n",
    "        wallets_clean = wallets_df[wallets_df['class'].isin([1, 2])].copy()\n",
    "        wallets_clean['class'] = wallets_clean['class'].map({1: 1, 2: 0})\n",
    "        \n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(wallets_clean['class']), y=wallets_clean['class'])\n",
    "        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "        \n",
    "        save_checkpoint('data_cleaned', {\n",
    "            'wallets_clean': wallets_clean,\n",
    "            'class_weight_dict': class_weight_dict\n",
    "        }, \"Data cleaning checkpoint saved\")\n",
    "\n",
    "    # STEP 4: Graph Construction with Centrality Features\n",
    "    if checkpoint_exists('graph_built_advanced'):\n",
    "        print(\"[RESUME] Loading advanced graph from checkpoint...\")\n",
    "        checkpoint_data = load_checkpoint('graph_built_advanced')\n",
    "        data = checkpoint_data['data']\n",
    "        addr_to_idx = checkpoint_data['addr_to_idx']\n",
    "        feature_cols = checkpoint_data['feature_cols']\n",
    "        scaler = checkpoint_data['scaler']\n",
    "        print(f\"[SUCCESS] Resumed with graph: {data.x.shape[0]} nodes, {data.edge_index.shape[1]} edges\")\n",
    "    else:\n",
    "        print(\"[INFO] Building advanced graph with centrality features...\")\n",
    "        \n",
    "        # Your existing graph construction\n",
    "        unique_addresses = wallets_clean['address'].unique()\n",
    "        addr_to_idx = {addr: idx for idx, addr in enumerate(unique_addresses)}\n",
    "        \n",
    "        exclude_cols = ['address', 'Time step', 'class']\n",
    "        feature_cols = [col for col in wallets_clean.columns if col not in exclude_cols]\n",
    "        \n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for addr in unique_addresses:\n",
    "            addr_data = wallets_clean[wallets_clean['address'] == addr].iloc[0]\n",
    "            features_list.append(addr_data[feature_cols].values)\n",
    "            labels_list.append(addr_data['class'])\n",
    "        \n",
    "        X = np.array(features_list)\n",
    "        y = np.array(labels_list)\n",
    "        \n",
    "        # Create edge list\n",
    "        edge_list = []\n",
    "        for _, row in edges_df.iterrows():\n",
    "            input_addr = row['input_address']\n",
    "            output_addr = row['output_address']\n",
    "            \n",
    "            if input_addr in addr_to_idx and output_addr in addr_to_idx:\n",
    "                input_idx = addr_to_idx[input_addr]\n",
    "                output_idx = addr_to_idx[output_addr]\n",
    "                edge_list.extend([[input_idx, output_idx], [output_idx, input_idx]])\n",
    "        \n",
    "        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # CRITICAL ADDITION: Compute centrality features\n",
    "        print(\"[ADVANCED] Computing centrality features...\")\n",
    "        centrality_features = compute_graph_centrality_features(edge_index, len(unique_addresses))\n",
    "        \n",
    "        # Combine original features with centrality features\n",
    "        centrality_matrix = np.column_stack([\n",
    "            centrality_features['pagerank'],\n",
    "            centrality_features['betweenness'],\n",
    "            # centrality_features['closeness'],\n",
    "            centrality_features['degree_centrality']\n",
    "        ])\n",
    "        \n",
    "        X_enhanced = np.concatenate([X, centrality_matrix], axis=1)\n",
    "        print(f\"[SUCCESS] Enhanced features: {X.shape[1]} -> {X_enhanced.shape[1]}\")\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = RobustScaler()\n",
    "        X_scaled = scaler.fit_transform(X_enhanced)\n",
    "        \n",
    "        # Create PyTorch tensors\n",
    "        x = torch.tensor(X_scaled, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        \n",
    "        save_checkpoint('graph_built_advanced', {\n",
    "            'data': data,\n",
    "            'addr_to_idx': addr_to_idx,\n",
    "            'feature_cols': feature_cols,\n",
    "            'scaler': scaler\n",
    "        }, \"Advanced graph construction checkpoint saved\")\n",
    "\n",
    "    # STEP 5: CRITICAL - Proper Train/Test Split (NO DATA LEAKAGE)\n",
    "    print(\"[CRITICAL FIX] Creating proper train/test split...\")\n",
    "    \n",
    "    # Node-level split to prevent data leakage\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(data.y)), \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=data.y.numpy()\n",
    "    )\n",
    "    \n",
    "    train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    print(f\"Train nodes: {train_mask.sum().item()}\")\n",
    "    print(f\"Test nodes: {test_mask.sum().item()}\")\n",
    "    \n",
    "    # STEP 6: Apply SMOTE to Training Data\n",
    "    print(\"[ADVANCED] Applying SMOTE to training data...\")\n",
    "    X_train = data.x[train_mask].cpu().numpy()\n",
    "    y_train = data.y[train_mask].cpu().numpy()\n",
    "    \n",
    "    # Apply SMOTE only if we have minority class samples\n",
    "    if sum(y_train) > 0 and sum(y_train) < len(y_train):\n",
    "        smote = SMOTE(random_state=42, k_neighbors=min(5, sum(y_train) - 1))\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "        print(f\"Original: {np.bincount(y_train)}\")\n",
    "        print(f\"SMOTE: {np.bincount(y_train_smote)}\")\n",
    "        \n",
    "        # Update training data in graph\n",
    "        # Note: This is complex with graph data, so we'll use class weights instead\n",
    "        print(\"[INFO] Using class weights due to graph structure complexity\")\n",
    "    \n",
    "    # STEP 7: Initialize Advanced Model\n",
    "    print(\"[ADVANCED] Initializing advanced GNN model...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "    \n",
    "    model = AdvancedGNN(\n",
    "        num_features=data.x.shape[1],\n",
    "        hidden_dim=128,\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    data = data.to(device)\n",
    "    \n",
    "    # STEP 8: Advanced Loss and Optimizer\n",
    "    print(\"[ADVANCED] Setting up Focal Loss and optimizer...\")\n",
    "    class_weights_tensor = torch.tensor([class_weight_dict[0], class_weight_dict[1]], dtype=torch.float).to(device)\n",
    "    \n",
    "    # Use Focal Loss instead of CrossEntropy\n",
    "    criterion = FocalLoss(alpha=1, gamma=2, weight=class_weights_tensor)\n",
    "    \n",
    "    # Advanced optimizer with different learning rates\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.gat_layers.parameters(), 'lr': 0.001},\n",
    "        {'params': model.gcn_layers.parameters(), 'lr': 0.001},\n",
    "        {'params': model.classifier.parameters(), 'lr': 0.002}\n",
    "    ], weight_decay=1e-4)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n",
    "    \n",
    "    # STEP 9: Create NeighborLoader\n",
    "    print(\"[INFO] Creating NeighborLoader...\")\n",
    "    train_loader = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=[15, 10, 5],\n",
    "        batch_size=512,\n",
    "        input_nodes=train_mask,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # STEP 10: Training Loop\n",
    "    print(\"[TRAINING] Starting advanced training...\")\n",
    "    best_f1 = 0\n",
    "    patience = 0\n",
    "    max_patience = 50\n",
    "    \n",
    "    for epoch in range(500):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            loss = criterion(out, batch.y)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluation every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            # CRITICAL: Use leakage-free evaluation\n",
    "            results = evaluate_without_leakage(model, data, test_idx, device, threshold=0.5)\n",
    "            \n",
    "            # Find optimal threshold\n",
    "            if len(np.unique(results['true_labels'])) > 1:\n",
    "                precision, recall, thresholds = precision_recall_curve(\n",
    "                    results['true_labels'], results['probabilities']\n",
    "                )\n",
    "                f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "                best_threshold_idx = np.argmax(f1_scores)\n",
    "                optimal_threshold = thresholds[best_threshold_idx] if len(thresholds) > 0 else 0.5\n",
    "                \n",
    "                # Re-evaluate with optimal threshold\n",
    "                results = evaluate_without_leakage(model, data, test_idx, device, threshold=optimal_threshold)\n",
    "                \n",
    "                test_f1 = f1_score(results['true_labels'], results['predictions'])\n",
    "                test_acc = np.mean(results['predictions'] == results['true_labels'])\n",
    "                \n",
    "                print(f'Epoch {epoch:03d}, Loss: {total_loss/num_batches:.4f}, '\n",
    "                      f'Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, '\n",
    "                      f'Threshold: {optimal_threshold:.3f}')\n",
    "                \n",
    "                # Early stopping\n",
    "                if test_f1 > best_f1:\n",
    "                    best_f1 = test_f1\n",
    "                    patience = 0\n",
    "                    torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_advanced_gnn.pth'))\n",
    "                else:\n",
    "                    patience += 1\n",
    "                    \n",
    "                if patience >= max_patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "    \n",
    "    # STEP 11: Final Evaluation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä FINAL EVALUATION (NO DATA LEAKAGE)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, 'best_advanced_gnn.pth')))\n",
    "    \n",
    "    # Final evaluation\n",
    "    results = evaluate_without_leakage(model, data, test_idx, device, threshold=optimal_threshold)\n",
    "    \n",
    "    test_acc = np.mean(results['predictions'] == results['true_labels'])\n",
    "    test_f1 = f1_score(results['true_labels'], results['predictions'])\n",
    "    auc_score = roc_auc_score(results['true_labels'], results['probabilities'])\n",
    "    \n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Final F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"Final AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(results['true_labels'], results['predictions'], \n",
    "                              target_names=['Licit', 'Illicit'], zero_division=0))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(results['true_labels'], results['predictions'])\n",
    "    print(f\"   Predicted:  Licit  Illicit\")\n",
    "    print(f\"   Licit:      {cm[0,0]:5d}     {cm[0,1]:4d}\")\n",
    "    print(f\"   Illicit:    {cm[1,0]:5d}     {cm[1,1]:4d}\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'scaler': scaler,\n",
    "        'addr_to_idx': addr_to_idx,\n",
    "        'feature_cols': feature_cols,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'class_weights': class_weight_dict,\n",
    "        'model_config': {\n",
    "            'num_features': data.x.shape[1],\n",
    "            'hidden_dim': 128,\n",
    "            'num_classes': 2,\n",
    "            'dropout': 0.3\n",
    "        },\n",
    "        'final_metrics': {\n",
    "            'test_accuracy': test_acc,\n",
    "            'auc_score': auc_score,\n",
    "            'f1_score': test_f1,\n",
    "            'optimal_threshold': optimal_threshold\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    save_checkpoint('final_advanced_model', model_data, \"Final advanced model saved\")\n",
    "    \n",
    "    print(\"\\nüéâ ADVANCED GNN TRAINING COMPLETED!\")\n",
    "    print(f\"‚úÖ Best F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"‚úÖ Final AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"‚úÖ Model saved to: {CHECKPOINT_DIR}/final_advanced_model.pkl\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'data': data,\n",
    "        'results': results,\n",
    "        'metrics': {\n",
    "            'accuracy': test_acc,\n",
    "            'f1_score': test_f1,\n",
    "            'auc_score': auc_score,\n",
    "            'threshold': optimal_threshold\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# INFERENCE FUNCTION FOR NEW DATA\n",
    "# ============================================================================\n",
    "def predict_new_addresses(model_checkpoint_path, new_addresses_data, scaler, addr_to_idx):\n",
    "    \"\"\"\n",
    "    Predict ransomware probability for new addresses\n",
    "    \"\"\"\n",
    "    print(\"üîç PREDICTING NEW ADDRESSES...\")\n",
    "    \n",
    "    # Load model\n",
    "    checkpoint_data = load_checkpoint('final_advanced_model')\n",
    "    model_config = checkpoint_data['model_config']\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AdvancedGNN(**model_config).to(device)\n",
    "    model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Process new data (assuming same format as training data)\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for addr_data in new_addresses_data:\n",
    "            # Scale features\n",
    "            features_scaled = scaler.transform(addr_data.reshape(1, -1))\n",
    "            features_tensor = torch.tensor(features_scaled, dtype=torch.float).to(device)\n",
    "            \n",
    "            # Note: For new addresses, we might not have graph structure\n",
    "            # This is a simplified prediction - in practice, you'd need to handle graph structure\n",
    "            # For now, using only the feature path of the model\n",
    "            \n",
    "            # Create dummy edge index for single node\n",
    "            edge_index = torch.tensor([[0], [0]], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Predict\n",
    "            output = model(features_tensor, edge_index)\n",
    "            prob = torch.softmax(output, dim=1)[0, 1].cpu().item()\n",
    "            \n",
    "            predictions.append({\n",
    "                'probability': prob,\n",
    "                'prediction': 1 if prob >= checkpoint_data['optimal_threshold'] else 0\n",
    "            })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON WITH BASELINE MODELS\n",
    "# ============================================================================\n",
    "def compare_with_baselines(data, train_idx, test_idx):\n",
    "    \"\"\"\n",
    "    Compare Advanced GNN with baseline models\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä COMPARING WITH BASELINE MODELS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    X_train = data.x[train_idx].cpu().numpy()\n",
    "    y_train = data.y[train_idx].cpu().numpy()\n",
    "    X_test = data.x[test_idx].cpu().numpy()\n",
    "    y_test = data.y[test_idx].cpu().numpy()\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "        'XGBoost': XGBClassifier(random_state=42, scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]))\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': acc,\n",
    "            'f1_score': f1,\n",
    "            'auc_score': auc\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} - Acc: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================================\n",
    "def analyze_feature_importance(model, data, feature_cols, device):\n",
    "    \"\"\"\n",
    "    Analyze which features are most important for the model\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get baseline prediction\n",
    "    with torch.no_grad():\n",
    "        baseline_output = model(data.x, data.edge_index)\n",
    "        baseline_probs = torch.softmax(baseline_output, dim=1)[:, 1]\n",
    "    \n",
    "    feature_importance = []\n",
    "    \n",
    "    # Permutation importance\n",
    "    for i in range(data.x.shape[1]):\n",
    "        # Permute feature i\n",
    "        data_permuted = data.x.clone()\n",
    "        data_permuted[:, i] = data_permuted[torch.randperm(data_permuted.shape[0]), i]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            permuted_output = model(data_permuted, data.edge_index)\n",
    "            permuted_probs = torch.softmax(permuted_output, dim=1)[:, 1]\n",
    "        \n",
    "        # Calculate importance as difference in predictions\n",
    "        importance = torch.mean(torch.abs(baseline_probs - permuted_probs)).item()\n",
    "        feature_importance.append(importance)\n",
    "    \n",
    "    # Sort by importance\n",
    "    #'closeness',\n",
    "    feature_names = feature_cols + ['pagerank', 'betweenness', 'degree_centrality']\n",
    "    importance_pairs = list(zip(feature_names, feature_importance))\n",
    "    importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    for i, (feature, importance) in enumerate(importance_pairs[:10]):\n",
    "        print(f\"{i+1:2d}. {feature:30s}: {importance:.6f}\")\n",
    "    \n",
    "    return importance_pairs\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Run the main training function\n",
    "        results = main_advanced_gnn_training()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üöÄ ADDITIONAL ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Feature importance analysis\n",
    "        if 'model' in results and 'data' in results:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            checkpoint_data = load_checkpoint('features_engineered')\n",
    "            feature_cols = load_checkpoint('graph_built_advanced')['feature_cols']\n",
    "            \n",
    "            importance_results = analyze_feature_importance(\n",
    "                results['model'], \n",
    "                results['data'], \n",
    "                feature_cols, \n",
    "                device\n",
    "            )\n",
    "        \n",
    "        print(\"\\nüéâ ALL ANALYSIS COMPLETED!\")\n",
    "        print(\"Check the checkpoints directory for saved models and data.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nüí° Check if all required data files are available and paths are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a1c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  import torch_geometric.typing\n",
      "C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ADVANCED GNN RANSOMWARE DETECTION WITH LEAKAGE FIXES\n",
      "======================================================================\n",
      "[RESUME] Loading data from checkpoint...\n",
      "[SUCCESS] Resumed with 1268260 wallet records and 2868964 edges\n",
      "[RESUME] Loading feature engineering from checkpoint...\n",
      "[SUCCESS] Resumed with 68 columns\n",
      "[RESUME] Loading cleaned data from checkpoint...\n",
      "[SUCCESS] Resumed with 367472 cleaned addresses\n",
      "[RESUME] Loading advanced graph from checkpoint...\n",
      "[SUCCESS] Resumed with graph: 265354 nodes, 2236444 edges\n",
      "[CRITICAL FIX] Creating proper train/test split...\n",
      "Train nodes: 212283\n",
      "Test nodes: 53071\n",
      "[INFO] SMOTE will not be directly applied to the graph.\n",
      "[INFO] Using class weights and Focal Loss to handle imbalance.\n",
      "[ADVANCED] Initializing advanced GNN model...\n",
      "[INFO] Using device: cuda\n",
      "[ADVANCED] Setting up Focal Loss and optimizer...\n",
      "[INFO] Creating NeighborLoader...\n",
      "[TRAINING] Starting advanced training...\n",
      "Epoch 000, Loss: 0.1545, Test Acc: 0.9546, F1: 0.5326, Threshold: 0.733\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, GraphSAGE, TransformerConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import networkx as nx\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mount Google Drive (if using Colab)\n",
    "try:\n",
    "    CHECKPOINT_DIR = 'Ransomware_Checkpoints_v2'\n",
    "except:\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL FIX 1: FOCAL LOSS FOR EXTREME IMBALANCE\n",
    "# ============================================================================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# ============================================================================\n",
    "# MEMORY OPTIMIZATION: BATCHED EVALUATION TO PREVENT OOM\n",
    "# ============================================================================\n",
    "def evaluate_in_batches(model, data, node_indices, device, threshold=0.5, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given set of nodes using mini-batches (NeighborLoader)\n",
    "    to prevent CUDA Out of Memory errors.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a NeighborLoader for the nodes to be evaluated\n",
    "    eval_loader = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=[15, 10, 5],  # Same neighborhood sampling as training\n",
    "        batch_size=batch_size,\n",
    "        input_nodes=node_indices,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            \n",
    "            # Note: The output 'out' is for all nodes in the batch (sampled neighbors included).\n",
    "            # We only care about the predictions for the primary 'input_nodes' of the batch.\n",
    "            # batch.batch_size gives the number of target nodes in the current batch.\n",
    "            target_out = out[:batch.batch_size]\n",
    "            \n",
    "            pred_proba = F.softmax(target_out, dim=1)[:, 1]\n",
    "            pred_binary = (pred_proba >= threshold).long()\n",
    "            \n",
    "            all_preds.append(pred_binary.cpu())\n",
    "            all_probs.append(pred_proba.cpu())\n",
    "            all_labels.append(batch.y[:batch.batch_size].cpu())\n",
    "\n",
    "    return {\n",
    "        'predictions': torch.cat(all_preds).numpy(),\n",
    "        'probabilities': torch.cat(all_probs).numpy(),\n",
    "        'true_labels': torch.cat(all_labels).numpy()\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ADVANCED FEATURE ENGINEERING: GRAPH CENTRALITY MEASURES\n",
    "# ============================================================================\n",
    "def compute_graph_centrality_features(edge_index, num_nodes):\n",
    "    \"\"\"Compute centrality measures for graph nodes\"\"\"\n",
    "    print(\"Computing graph centrality features...\")\n",
    "    \n",
    "    # Convert to NetworkX graph\n",
    "    edge_list = edge_index.t().cpu().numpy()\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    G.add_edges_from(edge_list)\n",
    "    \n",
    "    # Compute centrality measures\n",
    "    print(\"  - Computing PageRank...\")\n",
    "    pagerank = nx.pagerank(G, max_iter=50)\n",
    "    \n",
    "    print(\"  - Computing Betweenness Centrality...\")\n",
    "    betweenness = nx.betweenness_centrality(G, k=min(1000, num_nodes))\n",
    "    \n",
    "    # print(\"  - Computing Closeness Centrality...\")\n",
    "    # closeness = nx.closeness_centrality(G)\n",
    "    \n",
    "    print(\"  - Computing Degree Centrality...\")\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    pagerank_array = np.array([pagerank.get(i, 0) for i in range(num_nodes)])\n",
    "    betweenness_array = np.array([betweenness.get(i, 0) for i in range(num_nodes)])\n",
    "    # closeness_array = np.array([closeness.get(i, 0) for i in range(num_nodes)])\n",
    "    degree_array = np.array([degree_centrality.get(i, 0) for i in range(num_nodes)])\n",
    "    \n",
    "    return {\n",
    "        'pagerank': pagerank_array,\n",
    "        'betweenness': betweenness_array,\n",
    "        # 'closeness': closeness_array,\n",
    "        'degree_centrality': degree_array\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ADVANCED ARCHITECTURE: MULTI-LAYER GNN WITH ATTENTION FUSION\n",
    "# ============================================================================\n",
    "class AdvancedGNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super(AdvancedGNN, self).__init__()\n",
    "        \n",
    "        layer2_input_dim = (hidden_dim * 4) + hidden_dim\n",
    "        layer3_input_dim = (hidden_dim * 4) + hidden_dim\n",
    "\n",
    "        # Multiple GNN Types\n",
    "        self.gat_layers = nn.ModuleList([\n",
    "            GATConv(num_features, hidden_dim, heads=4, dropout=dropout),\n",
    "            GATConv(layer2_input_dim, hidden_dim, heads=4, dropout=dropout),\n",
    "            GATConv(layer3_input_dim, hidden_dim // 2, heads=2, dropout=dropout)\n",
    "        ])\n",
    "        \n",
    "        self.gcn_layers = nn.ModuleList([\n",
    "            GCNConv(num_features, hidden_dim),\n",
    "            GCNConv(layer2_input_dim, hidden_dim),\n",
    "            GCNConv(layer3_input_dim, hidden_dim // 2)\n",
    "        ])\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn_layers = nn.ModuleList([\n",
    "            nn.BatchNorm1d(hidden_dim * 4 + hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim * 4 + hidden_dim),\n",
    "            nn.BatchNorm1d((hidden_dim // 2 * 2) + (hidden_dim // 2))\n",
    "        ])\n",
    "        \n",
    "        # Residual connections\n",
    "        self.residual_projections = nn.ModuleList([\n",
    "            nn.Linear(num_features, layer2_input_dim),\n",
    "            nn.Linear(layer2_input_dim, layer3_input_dim),\n",
    "            nn.Linear(layer3_input_dim, (hidden_dim // 2 * 2) + (hidden_dim // 2))\n",
    "        ])\n",
    "        \n",
    "        # Feature-only path (XGBoost mimic)\n",
    "        self.feature_path = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Combined classifier\n",
    "        final_graph_dim = (hidden_dim // 2 * 2) + (hidden_dim // 2)\n",
    "        feature_only_dim = hidden_dim // 2\n",
    "        combined_dim = final_graph_dim + feature_only_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(hidden_dim // 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        original_x = x\n",
    "        current_x = x\n",
    "        \n",
    "        for i, (gat_layer, gcn_layer) in enumerate(zip(self.gat_layers, self.gcn_layers)):\n",
    "            input_for_residual = current_x\n",
    "            \n",
    "            h_gat = F.elu(gat_layer(current_x, edge_index))\n",
    "            h_gcn = F.elu(gcn_layer(current_x, edge_index))\n",
    "            \n",
    "            h_combined = torch.cat([h_gat, h_gcn], dim=1)\n",
    "            \n",
    "            residual = self.residual_projections[i](input_for_residual)\n",
    "            h_combined = h_combined + residual\n",
    "            h_combined = self.bn_layers[i](h_combined)\n",
    "            current_x = F.dropout(h_combined, p=self.dropout, training=self.training)\n",
    "        \n",
    "        graph_features = current_x\n",
    "        feature_only = self.feature_path(original_x)\n",
    "        combined = torch.cat([graph_features, feature_only], dim=1)\n",
    "        out = self.classifier(combined)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "# ============================================================================\n",
    "# CHECKPOINT SYSTEM (KEEPING YOUR EXISTING SYSTEM)\n",
    "# ============================================================================\n",
    "def save_checkpoint(checkpoint_name, data, message=\"Checkpoint saved\"):\n",
    "    \"\"\"Save checkpoint data\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"‚úÖ {message}: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_name):\n",
    "    \"\"\"Load checkpoint data\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def checkpoint_exists(checkpoint_name):\n",
    "    \"\"\"Check if checkpoint exists\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    return os.path.exists(checkpoint_path)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN INTEGRATION FUNCTION\n",
    "# ============================================================================\n",
    "def main_advanced_gnn_training():\n",
    "    \"\"\"Complete advanced GNN training with your existing structure\"\"\"\n",
    "    \n",
    "    print(\"üöÄ ADVANCED GNN RANSOMWARE DETECTION WITH LEAKAGE FIXES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # STEP 1: Data Loading (keeping your existing logic)\n",
    "    if checkpoint_exists('data_loaded'):\n",
    "        print(\"[RESUME] Loading data from checkpoint...\")\n",
    "        checkpoint_data = load_checkpoint('data_loaded')\n",
    "        wallets_df = checkpoint_data['wallets_df']\n",
    "        edges_df = checkpoint_data['edges_df']\n",
    "        print(f\"[SUCCESS] Resumed with {len(wallets_df)} wallet records and {len(edges_df)} edges\")\n",
    "    else:\n",
    "        print(\"[INFO] Loading wallet features and classes...\")\n",
    "        wallets_df = pd.read_csv('Elliptic++ Dataset/wallets_features_classes_combined.csv')\n",
    "        print(f\"[SUCCESS] Loaded {len(wallets_df)} wallet records\")\n",
    "\n",
    "        print(\"[INFO] Loading address-to-address edges...\")\n",
    "        edges_df = pd.read_csv('Elliptic++ Dataset/AddrAddr_edgelist.csv')\n",
    "        print(f\"[SUCCESS] Loaded {len(edges_df)} edges\")\n",
    "\n",
    "        save_checkpoint('data_loaded', {\n",
    "            'wallets_df': wallets_df,\n",
    "            'edges_df': edges_df\n",
    "        }, \"Data loading checkpoint saved\")\n",
    "\n",
    "    # STEP 2: Feature Engineering (using your enhanced features)\n",
    "    def create_enhanced_pattern_features(df):\n",
    "        \"\"\"Create the EXACT same enhanced features as XGBoost model\"\"\"\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "        # Your existing feature engineering...\n",
    "        df['partner_transaction_ratio'] = (df.get('transacted_w_address_total', 0) / (df.get('total_txs', 1) + 1e-8))\n",
    "        df['activity_density'] = (df.get('total_txs', 0) / (df.get('lifetime_in_blocks', 1) + 1e-8))\n",
    "        df['transaction_size_variance'] = ((df.get('btc_transacted_max', 0) - df.get('btc_transacted_min', 0)) / (df.get('btc_transacted_mean', 1) + 1e-8))\n",
    "        df['flow_imbalance'] = ((df.get('btc_sent_total', 0) - df.get('btc_received_total', 0)) / (df.get('btc_transacted_total', 1) + 1e-8))\n",
    "        df['temporal_spread'] = ((df.get('last_block_appeared_in', 0) - df.get('first_block_appeared_in', 0)) / (df.get('num_timesteps_appeared_in', 1) + 1e-8))\n",
    "        df['fee_percentile'] = (df.get('fees_total', 0) / (df.get('btc_transacted_total', 1) + 1e-8))\n",
    "        df['interaction_intensity'] = (df.get('num_addr_transacted_multiple', 0) / (df.get('transacted_w_address_total', 1) + 1e-8))\n",
    "        df['value_per_transaction'] = (df.get('btc_transacted_total', 0) / (df.get('total_txs', 1) + 1e-8))\n",
    "        df['burst_activity'] = (df.get('total_txs', 0) * df.get('activity_density', 0))\n",
    "        df['mixing_intensity'] = (df.get('partner_transaction_ratio', 0) * df.get('interaction_intensity', 0))\n",
    "        return df\n",
    "\n",
    "    if checkpoint_exists('features_engineered'):\n",
    "        print(\"[RESUME] Loading feature engineering from checkpoint...\")\n",
    "        checkpoint_data = load_checkpoint('features_engineered')\n",
    "        wallets_df = checkpoint_data['wallets_df']\n",
    "        print(f\"[SUCCESS] Resumed with {len(wallets_df.columns)} columns\")\n",
    "    else:\n",
    "        print(\"[INFO] Creating enhanced features...\")\n",
    "        wallets_df = create_enhanced_pattern_features(wallets_df)\n",
    "        print(f\"[SUCCESS] Enhanced features created. Now have {len(wallets_df.columns)} columns\")\n",
    "        save_checkpoint('features_engineered', {'wallets_df': wallets_df}, \"Feature engineering checkpoint saved\")\n",
    "\n",
    "    # STEP 3: Data Cleaning (keeping your existing logic)\n",
    "    if checkpoint_exists('data_cleaned'):\n",
    "        print(\"[RESUME] Loading cleaned data from checkpoint...\")\n",
    "        checkpoint_data = load_checkpoint('data_cleaned')\n",
    "        wallets_clean = checkpoint_data['wallets_clean']\n",
    "        class_weight_dict = checkpoint_data['class_weight_dict']\n",
    "        print(f\"[SUCCESS] Resumed with {len(wallets_clean)} cleaned addresses\")\n",
    "    else:\n",
    "        print(\"[INFO] Cleaning data...\")\n",
    "        wallets_clean = wallets_df[wallets_df['class'].isin([1, 2])].copy()\n",
    "        wallets_clean['class'] = wallets_clean['class'].map({1: 1, 2: 0})\n",
    "        \n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(wallets_clean['class']), y=wallets_clean['class'])\n",
    "        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "        \n",
    "        save_checkpoint('data_cleaned', {\n",
    "            'wallets_clean': wallets_clean,\n",
    "            'class_weight_dict': class_weight_dict\n",
    "        }, \"Data cleaning checkpoint saved\")\n",
    "\n",
    "    # STEP 4: Graph Construction with Centrality Features\n",
    "    if checkpoint_exists('graph_built_advanced'):\n",
    "        print(\"[RESUME] Loading advanced graph from checkpoint...\")\n",
    "        checkpoint_data = load_checkpoint('graph_built_advanced')\n",
    "        data = checkpoint_data['data']\n",
    "        addr_to_idx = checkpoint_data['addr_to_idx']\n",
    "        feature_cols = checkpoint_data['feature_cols']\n",
    "        scaler = checkpoint_data['scaler']\n",
    "        print(f\"[SUCCESS] Resumed with graph: {data.x.shape[0]} nodes, {data.edge_index.shape[1]} edges\")\n",
    "    else:\n",
    "        print(\"[INFO] Building advanced graph with centrality features...\")\n",
    "        \n",
    "        unique_addresses = wallets_clean['address'].unique()\n",
    "        addr_to_idx = {addr: idx for idx, addr in enumerate(unique_addresses)}\n",
    "        \n",
    "        exclude_cols = ['address', 'Time step', 'class']\n",
    "        feature_cols = [col for col in wallets_clean.columns if col not in exclude_cols]\n",
    "        \n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for addr in unique_addresses:\n",
    "            addr_data = wallets_clean[wallets_clean['address'] == addr].iloc[0]\n",
    "            features_list.append(addr_data[feature_cols].values)\n",
    "            labels_list.append(addr_data['class'])\n",
    "        \n",
    "        X = np.array(features_list)\n",
    "        y = np.array(labels_list)\n",
    "        \n",
    "        edge_list = []\n",
    "        for _, row in edges_df.iterrows():\n",
    "            input_addr, output_addr = row['input_address'], row['output_address']\n",
    "            if input_addr in addr_to_idx and output_addr in addr_to_idx:\n",
    "                input_idx, output_idx = addr_to_idx[input_addr], addr_to_idx[output_addr]\n",
    "                edge_list.extend([[input_idx, output_idx], [output_idx, input_idx]])\n",
    "        \n",
    "        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        print(\"[ADVANCED] Computing centrality features...\")\n",
    "        centrality_features = compute_graph_centrality_features(edge_index, len(unique_addresses))\n",
    "        \n",
    "        centrality_matrix = np.column_stack([\n",
    "            centrality_features['pagerank'],\n",
    "            centrality_features['betweenness'],\n",
    "            centrality_features['degree_centrality']\n",
    "        ])\n",
    "        \n",
    "        X_enhanced = np.concatenate([X, centrality_matrix], axis=1)\n",
    "        print(f\"[SUCCESS] Enhanced features: {X.shape[1]} -> {X_enhanced.shape[1]}\")\n",
    "        \n",
    "        scaler = RobustScaler()\n",
    "        X_scaled = scaler.fit_transform(X_enhanced)\n",
    "        \n",
    "        x = torch.tensor(X_scaled, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        \n",
    "        save_checkpoint('graph_built_advanced', {\n",
    "            'data': data, 'addr_to_idx': addr_to_idx,\n",
    "            'feature_cols': feature_cols, 'scaler': scaler\n",
    "        }, \"Advanced graph construction checkpoint saved\")\n",
    "\n",
    "    # STEP 5: CRITICAL - Proper Train/Test Split (NO DATA LEAKAGE)\n",
    "    print(\"[CRITICAL FIX] Creating proper train/test split...\")\n",
    "    \n",
    "    indices = range(len(data.y))\n",
    "    train_idx, test_idx, y_train, y_test = train_test_split(\n",
    "        indices, data.y.numpy(), test_size=0.2, random_state=42, stratify=data.y.numpy()\n",
    "    )\n",
    "    \n",
    "    train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    print(f\"Train nodes: {len(train_idx)}\")\n",
    "    print(f\"Test nodes: {len(test_idx)}\")\n",
    "    \n",
    "    # STEP 6: SMOTE Info\n",
    "    print(\"[INFO] SMOTE will not be directly applied to the graph.\")\n",
    "    print(\"[INFO] Using class weights and Focal Loss to handle imbalance.\")\n",
    "\n",
    "    # STEP 7: Initialize Advanced Model\n",
    "    print(\"[ADVANCED] Initializing advanced GNN model...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "    \n",
    "    model = AdvancedGNN(\n",
    "        num_features=data.x.shape[1], hidden_dim=128, dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    data = data.to(device)\n",
    "    \n",
    "    # STEP 8: Advanced Loss and Optimizer\n",
    "    print(\"[ADVANCED] Setting up Focal Loss and optimizer...\")\n",
    "    class_weights_tensor = torch.tensor([class_weight_dict[0], class_weight_dict[1]], dtype=torch.float).to(device)\n",
    "    \n",
    "    criterion = FocalLoss(alpha=1, gamma=2, weight=class_weights_tensor)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.gat_layers.parameters(), 'lr': 0.001},\n",
    "        {'params': model.gcn_layers.parameters(), 'lr': 0.001},\n",
    "        {'params': model.classifier.parameters(), 'lr': 0.002}\n",
    "    ], weight_decay=1e-4)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n",
    "    \n",
    "    # STEP 9: Create NeighborLoader\n",
    "    print(\"[INFO] Creating NeighborLoader...\")\n",
    "    train_loader = NeighborLoader(\n",
    "        data, num_neighbors=[15, 10, 5], batch_size=512,\n",
    "        input_nodes=train_mask, shuffle=True, num_workers=0\n",
    "    )\n",
    "    \n",
    "    # STEP 10: Training Loop\n",
    "    print(\"[TRAINING] Starting advanced training...\")\n",
    "    best_f1 = 0\n",
    "    patience = 0\n",
    "    max_patience = 50\n",
    "    optimal_threshold = 0.5\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            loss = criterion(out[:batch.batch_size], batch.y[:batch.batch_size])\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            torch.cuda.empty_cache() # Free memory before evaluation\n",
    "            # OPTIMIZATION: Use batched evaluation\n",
    "            results = evaluate_in_batches(model, data, test_mask, device)\n",
    "            \n",
    "            if len(np.unique(results['true_labels'])) > 1:\n",
    "                precision, recall, thresholds = precision_recall_curve(\n",
    "                    results['true_labels'], results['probabilities']\n",
    "                )\n",
    "                f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "                f1_scores = np.nan_to_num(f1_scores) # Handle division by zero\n",
    "                best_threshold_idx = np.argmax(f1_scores)\n",
    "                current_optimal_threshold = thresholds[best_threshold_idx] if len(thresholds) > 0 else 0.5\n",
    "                \n",
    "                # Re-evaluate with this optimal threshold for logging\n",
    "                temp_results = evaluate_in_batches(model, data, test_mask, device, threshold=current_optimal_threshold)\n",
    "                test_f1 = f1_score(temp_results['true_labels'], temp_results['predictions'])\n",
    "                test_acc = np.mean(temp_results['predictions'] == temp_results['true_labels'])\n",
    "                \n",
    "                print(f'Epoch {epoch:03d}, Loss: {total_loss/num_batches:.4f}, '\n",
    "                      f'Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, '\n",
    "                      f'Threshold: {current_optimal_threshold:.3f}')\n",
    "                \n",
    "                if test_f1 > best_f1:\n",
    "                    best_f1 = test_f1\n",
    "                    optimal_threshold = current_optimal_threshold # Save the best threshold\n",
    "                    patience = 0\n",
    "                    torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_advanced_gnn.pth'))\n",
    "                else:\n",
    "                    patience += 1\n",
    "                \n",
    "                if patience >= max_patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            torch.cuda.empty_cache() # Free memory after evaluation\n",
    "\n",
    "    # STEP 11: Final Evaluation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä FINAL EVALUATION (NO DATA LEAKAGE)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, 'best_advanced_gnn.pth')))\n",
    "    \n",
    "    # Final evaluation using the best threshold found during training\n",
    "    results = evaluate_in_batches(model, data, test_mask, device, threshold=optimal_threshold)\n",
    "    \n",
    "    test_acc = np.mean(results['predictions'] == results['true_labels'])\n",
    "    test_f1 = f1_score(results['true_labels'], results['predictions'])\n",
    "    auc_score = roc_auc_score(results['true_labels'], results['probabilities'])\n",
    "    \n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Final F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"Final AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"Optimal Threshold Used: {optimal_threshold:.3f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(results['true_labels'], results['predictions'], \n",
    "                                target_names=['Licit', 'Illicit'], zero_division=0))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(results['true_labels'], results['predictions'])\n",
    "    print(f\"   Predicted:  Licit  Illicit\")\n",
    "    print(f\"   Actual Licit:   {cm[0,0]:5d}   {cm[0,1]:5d}\")\n",
    "    print(f\"   Actual Illicit: {cm[1,0]:5d}   {cm[1,1]:5d}\")\n",
    "\n",
    "    \n",
    "    # Save final model\n",
    "    model_data = {\n",
    "        'model_state_dict': model.state_dict(), 'scaler': scaler,\n",
    "        'addr_to_idx': addr_to_idx, 'feature_cols': feature_cols,\n",
    "        'optimal_threshold': optimal_threshold, 'class_weights': class_weight_dict,\n",
    "        'model_config': {\n",
    "            'num_features': data.num_features, 'hidden_dim': 128,\n",
    "            'num_classes': 2, 'dropout': 0.3\n",
    "        },\n",
    "        'final_metrics': {\n",
    "            'test_accuracy': test_acc, 'auc_score': auc_score,\n",
    "            'f1_score': test_f1, 'optimal_threshold': optimal_threshold\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    save_checkpoint('final_advanced_model', model_data, \"Final advanced model saved\")\n",
    "    \n",
    "    print(\"\\nüéâ ADVANCED GNN TRAINING COMPLETED!\")\n",
    "    print(f\"‚úÖ Best F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"‚úÖ Final AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"‚úÖ Model saved to: {CHECKPOINT_DIR}/final_advanced_model.pkl\")\n",
    "    \n",
    "    return {\n",
    "        'model': model, 'data': data, 'results': results,\n",
    "        'metrics': {\n",
    "            'accuracy': test_acc, 'f1_score': test_f1,\n",
    "            'auc_score': auc_score, 'threshold': optimal_threshold\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS (CPU OPTIMIZED)\n",
    "# ============================================================================\n",
    "def analyze_feature_importance(model, data, feature_cols, device):\n",
    "    \"\"\"\n",
    "    Analyze feature importance on the CPU to prevent VRAM issues.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç FEATURE IMPORTANCE ANALYSIS (Running on CPU)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Move model and data to CPU for this analysis\n",
    "    model.to('cpu')\n",
    "    data.to('cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    # Get baseline prediction\n",
    "    with torch.no_grad():\n",
    "        baseline_output = model(data.x, data.edge_index)\n",
    "        baseline_probs = torch.softmax(baseline_output, dim=1)[:, 1]\n",
    "    \n",
    "    feature_importance = []\n",
    "    \n",
    "    # Permutation importance\n",
    "    for i in range(data.x.shape[1]):\n",
    "        original_col = data.x[:, i].clone()\n",
    "        \n",
    "        # Permute feature i\n",
    "        permuted_indices = torch.randperm(data.x.shape[0])\n",
    "        data.x[:, i] = data.x[permuted_indices, i]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            permuted_output = model(data.x, data.edge_index)\n",
    "            permuted_probs = torch.softmax(permuted_output, dim=1)[:, 1]\n",
    "        \n",
    "        # Calculate importance and restore original column\n",
    "        importance = torch.mean(torch.abs(baseline_probs - permuted_probs)).item()\n",
    "        feature_importance.append(importance)\n",
    "        data.x[:, i] = original_col # Restore for next iteration\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"  ... analyzed {i+1}/{data.x.shape[1]} features\")\n",
    "\n",
    "    # Move model back to original device\n",
    "    model.to(device)\n",
    "\n",
    "    # Sort by importance\n",
    "    feature_names = feature_cols + ['pagerank', 'betweenness', 'degree_centrality']\n",
    "    importance_pairs = list(zip(feature_names, feature_importance))\n",
    "    importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    for i, (feature, importance) in enumerate(importance_pairs[:10]):\n",
    "        print(f\"{i+1:2d}. {feature:30s}: {importance:.6f}\")\n",
    "    \n",
    "    return importance_pairs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Run the main training function\n",
    "        results = main_advanced_gnn_training()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üöÄ ADDITIONAL ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Feature importance analysis\n",
    "        if 'model' in results and 'data' in results:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            # We need the original, un-enhanced feature columns list\n",
    "            graph_checkpoint = load_checkpoint('graph_built_advanced')\n",
    "            if graph_checkpoint:\n",
    "                 feature_cols = graph_checkpoint['feature_cols']\n",
    "                 importance_results = analyze_feature_importance(\n",
    "                    results['model'], \n",
    "                    results['data'], \n",
    "                    feature_cols, \n",
    "                    device\n",
    "                )\n",
    "        \n",
    "        print(\"\\nüéâ ALL ANALYSIS COMPLETED!\")\n",
    "        print(\"Check the checkpoints directory for saved models and data.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nüí° Check if all required data files are available and paths are correct.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
