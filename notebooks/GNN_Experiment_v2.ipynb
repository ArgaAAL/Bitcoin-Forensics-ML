{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a096467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary library to download files from Google Drive\n",
    "!pip install gdown\n",
    "\n",
    "# Download the shared folder\n",
    "!gdown --folder https://drive.google.com/drive/folders/1MRPXz79Lu_JGLlJ21MDfML44dKN9R08l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa12160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aee355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 (New Cell)\n",
    "def check_tensor(tensor, name=\"Tensor\"):\n",
    "    \"\"\"Checks a tensor for NaN or Inf values.\"\"\"\n",
    "    if torch.isnan(tensor).any() or torch.isinf(tensor).any():\n",
    "        print(f\"ðŸ”´ ALERT: '{name}' contains NaN or Inf values!\")\n",
    "        return True\n",
    "    else:\n",
    "        # This print statement is optional, you can comment it out if the log is too noisy\n",
    "        print(f\"âœ… OK: '{name}' is clean.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "# Re-import NeighborLoader for mini-batch training\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CHECKPOINT SYSTEM\n",
    "CHECKPOINT_DIR = 'Ransomware_Checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(checkpoint_name, data, message=\"Checkpoint saved\"):\n",
    "    \"\"\"Save checkpoint data\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"âœ… {message}: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_name):\n",
    "    \"\"\"Load checkpoint data\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def checkpoint_exists(checkpoint_name):\n",
    "    \"\"\"Check if checkpoint exists\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    return os.path.exists(checkpoint_path)\n",
    "\n",
    "# Updated title for clarity\n",
    "print(\"ðŸš€ IMPROVED GNN RANSOMWARE DETECTION MODEL WITH NEIGHBORLOADER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STEP 1: Check for data loading checkpoint\n",
    "if checkpoint_exists('data_loaded'):\n",
    "    print(\"[RESUME] Loading data from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('data_loaded')\n",
    "    wallets_df = checkpoint_data['wallets_df']\n",
    "    edges_df = checkpoint_data['edges_df']\n",
    "    print(f\"[SUCCESS] Resumed with {len(wallets_df)} wallet records and {len(edges_df)} edges\")\n",
    "else:\n",
    "    print(\"[INFO] Loading wallet features and classes...\")\n",
    "    wallets_df = pd.read_csv('Elliptic++ Dataset/wallets_features_classes_combined.csv')\n",
    "    print(f\"[SUCCESS] Loaded {len(wallets_df)} wallet records\")\n",
    "\n",
    "    print(\"[INFO] Loading address-to-address edges...\")\n",
    "    edges_df = pd.read_csv('Elliptic++ Dataset/AddrAddr_edgelist.csv')\n",
    "    print(f\"[SUCCESS] Loaded {len(edges_df)} edges\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('data_loaded', {\n",
    "        'wallets_df': wallets_df,\n",
    "        'edges_df': edges_df\n",
    "    }, \"Data loading checkpoint saved\")\n",
    "\n",
    "# STEP 2: Check for feature engineering checkpoint\n",
    "def create_enhanced_pattern_features(df):\n",
    "    \"\"\"Create the EXACT same enhanced features as XGBoost model\"\"\"\n",
    "\n",
    "    # Fill missing values first (same as XGBoost)\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "\n",
    "    # 1. Partner Transaction Ratio (connectivity)\n",
    "    df['partner_transaction_ratio'] = (\n",
    "        df.get('transacted_w_address_total', 0) /\n",
    "        (df.get('total_txs', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 2. Activity Density (txs per block)\n",
    "    df['activity_density'] = (\n",
    "        df.get('total_txs', 0) /\n",
    "        (df.get('lifetime_in_blocks', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 3. Transaction Size Variance (volatility)\n",
    "    df['transaction_size_variance'] = (\n",
    "        df.get('btc_transacted_max', 0) - df.get('btc_transacted_min', 0)\n",
    "    ) / (df.get('btc_transacted_mean', 1) + 1e-8)\n",
    "\n",
    "    # 4. Flow Imbalance (money laundering indicator)\n",
    "    df['flow_imbalance'] = (\n",
    "        (df.get('btc_sent_total', 0) - df.get('btc_received_total', 0)) /\n",
    "        (df.get('btc_transacted_total', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 5. Temporal Spread (time pattern)\n",
    "    df['temporal_spread'] = (\n",
    "        df.get('last_block_appeared_in', 0) - df.get('first_block_appeared_in', 0)\n",
    "    ) / (df.get('num_timesteps_appeared_in', 1) + 1e-8)\n",
    "\n",
    "    # 6. Fee Percentile (urgency indicator)\n",
    "    df['fee_percentile'] = (\n",
    "        df.get('fees_total', 0) /\n",
    "        (df.get('btc_transacted_total', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 7. Interaction Intensity (network centrality)\n",
    "    df['interaction_intensity'] = (\n",
    "        df.get('num_addr_transacted_multiple', 0) /\n",
    "        (df.get('transacted_w_address_total', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 8. Value Per Transaction (transaction size)\n",
    "    df['value_per_transaction'] = (\n",
    "        df.get('btc_transacted_total', 0) /\n",
    "        (df.get('total_txs', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 9. RANSOMWARE-SPECIFIC: Burst Activity (rapid txs)\n",
    "    df['burst_activity'] = (\n",
    "        df.get('total_txs', 0) * df.get('activity_density', 0)\n",
    "    )\n",
    "\n",
    "    # 10. RANSOMWARE-SPECIFIC: Mixing Intensity (obfuscation)\n",
    "    df['mixing_intensity'] = (\n",
    "        df.get('partner_transaction_ratio', 0) * df.get('interaction_intensity', 0)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "if checkpoint_exists('features_engineered'):\n",
    "    print(\"[RESUME] Loading feature engineering from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('features_engineered')\n",
    "    wallets_df = checkpoint_data['wallets_df']\n",
    "    print(f\"[SUCCESS] Resumed with {len(wallets_df.columns)} columns\")\n",
    "else:\n",
    "    print(\"[INFO] Creating enhanced features...\")\n",
    "    wallets_df = create_enhanced_pattern_features(wallets_df)\n",
    "    print(f\"[SUCCESS] Enhanced features created. Now have {len(wallets_df.columns)} columns\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('features_engineered', {\n",
    "        'wallets_df': wallets_df\n",
    "    }, \"Feature engineering checkpoint saved\")\n",
    "\n",
    "# STEP 3: Check for data cleaning checkpoint\n",
    "if checkpoint_exists('data_cleaned'):\n",
    "    print(\"[RESUME] Loading cleaned data from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('data_cleaned')\n",
    "    wallets_clean = checkpoint_data['wallets_clean']\n",
    "    class_weight_dict = checkpoint_data['class_weight_dict']\n",
    "    print(f\"[SUCCESS] Resumed with {len(wallets_clean)} cleaned addresses\")\n",
    "    print(f\"[INFO] Class distribution: {wallets_clean['class'].value_counts().to_dict()}\")\n",
    "    print(f\"[INFO] Class weights: {class_weight_dict}\")\n",
    "else:\n",
    "    print(\"[INFO] Cleaning data with same logic as XGBoost...\")\n",
    "\n",
    "    # Keep only labeled addresses (1=Illicit, 2=Licit)\n",
    "    wallets_clean = wallets_df[wallets_df['class'].isin([1, 2])].copy()\n",
    "    print(f\"[INFO] Labeled addresses: {len(wallets_clean)}\")\n",
    "\n",
    "    # Remap classes: 1->1 (Illicit), 2->0 (Licit) - SAME AS XGBOOST\n",
    "    wallets_clean['class'] = wallets_clean['class'].map({1: 1, 2: 0})\n",
    "    print(f\"[INFO] Class distribution: {wallets_clean['class'].value_counts().to_dict()}\")\n",
    "\n",
    "    # Calculate class weights (same as XGBoost)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(wallets_clean['class']), y=wallets_clean['class'])\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    print(f\"[INFO] Class weights: {class_weight_dict}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('data_cleaned', {\n",
    "        'wallets_clean': wallets_clean,\n",
    "        'class_weight_dict': class_weight_dict\n",
    "    }, \"Data cleaning checkpoint saved\")\n",
    "\n",
    "# STEP 4: Check for graph construction checkpoint\n",
    "if checkpoint_exists('graph_built'):\n",
    "    print(\"[RESUME] Loading graph from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('graph_built')\n",
    "    data = checkpoint_data['data']\n",
    "    addr_to_idx = checkpoint_data['addr_to_idx']\n",
    "    feature_cols = checkpoint_data['feature_cols']\n",
    "    scaler = checkpoint_data['scaler']\n",
    "    X_scaled = checkpoint_data['X_scaled']\n",
    "    print(f\"[SUCCESS] Resumed with graph: {data.x.shape[0]} nodes, {data.edge_index.shape[1]} edges\")\n",
    "else:\n",
    "    print(\"[INFO] Building improved graph structure...\")\n",
    "    print(\"This is the slow part - creating address mappings and features...\")\n",
    "\n",
    "    # Create address to index mapping\n",
    "    unique_addresses = wallets_clean['address'].unique()\n",
    "    addr_to_idx = {addr: idx for idx, addr in enumerate(unique_addresses)}\n",
    "\n",
    "    # Prepare features and labels\n",
    "    exclude_cols = ['address', 'Time step', 'class']\n",
    "    feature_cols = [col for col in wallets_clean.columns if col not in exclude_cols]\n",
    "\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    address_list = []\n",
    "\n",
    "    print(f\"[INFO] Processing {len(unique_addresses)} unique addresses...\")\n",
    "    for i, addr in enumerate(unique_addresses):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"  Progress: {i}/{len(unique_addresses)} ({i/len(unique_addresses)*100:.1f}%)\")\n",
    "\n",
    "        addr_data = wallets_clean[wallets_clean['address'] == addr].iloc[0]\n",
    "        features_list.append(addr_data[feature_cols].values)\n",
    "        labels_list.append(addr_data['class'])\n",
    "        address_list.append(addr)\n",
    "\n",
    "    X = np.array(features_list)\n",
    "    y = np.array(labels_list)\n",
    "\n",
    "    print(f\"[INFO] Feature matrix shape: {X.shape}\")\n",
    "    print(f\"[INFO] Labels shape: {y.shape}\")\n",
    "\n",
    "    # Create edge list with weights\n",
    "    print(\"[INFO] Creating weighted edges...\")\n",
    "    edge_list = []\n",
    "    edge_weights = []\n",
    "\n",
    "    print(f\"[INFO] Processing {len(edges_df)} edges...\")\n",
    "    for i, (_, row) in enumerate(edges_df.iterrows()):\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"  Edge progress: {i}/{len(edges_df)} ({i/len(edges_df)*100:.1f}%)\")\n",
    "\n",
    "        input_addr = row['input_address']\n",
    "        output_addr = row['output_address']\n",
    "\n",
    "        if input_addr in addr_to_idx and output_addr in addr_to_idx:\n",
    "            input_idx = addr_to_idx[input_addr]\n",
    "            output_idx = addr_to_idx[output_addr]\n",
    "\n",
    "            # Add both directions for undirected graph\n",
    "            edge_list.extend([[input_idx, output_idx], [output_idx, input_idx]])\n",
    "\n",
    "            # Use transaction amount as edge weight if available\n",
    "            weight = row.get('value', 1.0) if 'value' in row else 1.0\n",
    "            edge_weights.extend([weight, weight])\n",
    "\n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "    print(f\"[INFO] Graph edges: {edge_index.shape[1]} edges (bidirectional)\")\n",
    "\n",
    "    # Scale features\n",
    "    print(\"[INFO] Scaling features with RobustScaler...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.tensor(X_scaled, dtype=torch.float)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    print(f\"[INFO] Final graph structure:\")\n",
    "    print(f\"  - Nodes: {x.shape[0]}\")\n",
    "    print(f\"  - Node features: {x.shape[1]}\")\n",
    "    f\"  - Edges: {edge_index.shape[1]}\"\n",
    "    print(f\"  - Classes: Licit={sum(y==0)}, Illicit={sum(y==1)}\")\n",
    "\n",
    "    # Create PyTorch Geometric data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_weights, y=y)\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('graph_built', {\n",
    "        'data': data,\n",
    "        'addr_to_idx': addr_to_idx,\n",
    "        'feature_cols': feature_cols,\n",
    "        'scaler': scaler,\n",
    "        'X_scaled': X_scaled\n",
    "    }, \"Graph construction checkpoint saved\")\n",
    "\n",
    "# STEP 5: Model Definition (Updated for batch processing and torch.nn.BatchNorm1d)\n",
    "class ImprovedGNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super(ImprovedGNN, self).__init__()\n",
    "\n",
    "        # Graph attention layers for better feature learning\n",
    "        self.gat1 = GATConv(num_features, hidden_dim, heads=4, dropout=dropout)\n",
    "        # Use standard torch.nn.BatchNorm1d\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim * 4)\n",
    "\n",
    "        self.gat2 = GATConv(hidden_dim * 4, hidden_dim, heads=4, dropout=dropout)\n",
    "        # Use standard torch.nn.BatchNorm1d\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim * 4)\n",
    "\n",
    "        self.gat3 = GATConv(hidden_dim * 4, hidden_dim // 2, heads=2, dropout=dropout)\n",
    "        # Use standard torch.nn.BatchNorm1d\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim) # output shape after gat3 is hidden_dim // 2 * 2 = hidden_dim\n",
    "\n",
    "        # Feature-only path (like XGBoost)\n",
    "        self.feature_path = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Combined classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim // 2),  # Graph + Feature paths\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Graph path\n",
    "        h1 = F.elu(self.gat1(x, edge_index))\n",
    "        h1 = self.bn1(h1)\n",
    "        h1 = F.dropout(h1, p=self.dropout, training=self.training)\n",
    "\n",
    "        h2 = F.elu(self.gat2(h1, edge_index))\n",
    "        h2 = self.bn2(h2)\n",
    "        h2 = F.dropout(h2, p=self.dropout, training=self.training)\n",
    "\n",
    "        h3 = F.elu(self.gat3(h2, edge_index))\n",
    "        h3 = self.bn3(h3)\n",
    "        graph_features = F.dropout(h3, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Feature-only path (mimics XGBoost)\n",
    "        feature_only = self.feature_path(x)\n",
    "\n",
    "        # Combine both paths\n",
    "        combined = torch.cat([graph_features, feature_only], dim=1)\n",
    "\n",
    "        # Classification\n",
    "        out = self.classifier(combined)\n",
    "\n",
    "        return out\n",
    "\n",
    "# STEP 6: Training setup with NeighborLoader - FIXED CUDA DETECTION\n",
    "def save_training_checkpoint(epoch, model, optimizer, scheduler, best_f1, patience_counter, train_losses, test_f1_scores):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    training_state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'best_f1': best_f1,\n",
    "        'patience_counter': patience_counter,\n",
    "        'train_losses': train_losses,\n",
    "        'test_f1_scores': test_f1_scores\n",
    "    }\n",
    "    torch.save(training_state, os.path.join(CHECKPOINT_DIR, 'training_checkpoint.pth'))\n",
    "    print(f\"ðŸ’¾ Training checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "# FORCE CUDA DETECTION - Don't load device from checkpoint\n",
    "print(\"[INFO] Forcing CUDA device detection...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[INFO] CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[INFO] CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"[INFO] CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Check for training setup checkpoint - but don't load device from it\n",
    "if checkpoint_exists('training_setup'):\n",
    "    print(\"[RESUME] Loading training setup from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('training_setup')\n",
    "    train_mask = checkpoint_data['train_mask']\n",
    "    test_mask = checkpoint_data['test_mask']\n",
    "    # DON'T LOAD DEVICE FROM CHECKPOINT - use current detection\n",
    "    print(f\"[SUCCESS] Resumed training setup\")\n",
    "    print(f\"[INFO] Train samples: {train_mask.sum()}\")\n",
    "    print(f\"[INFO] Test samples: {test_mask.sum()}\")\n",
    "    print(f\"[INFO] Device: {device}\")  # Use current device\n",
    "else:\n",
    "    print(\"[INFO] Creating stratified train-test split...\")\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(data.y)), test_size=0.2, random_state=42,\n",
    "        stratify=data.y.numpy()\n",
    "    )\n",
    "\n",
    "    train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "\n",
    "    print(f\"[INFO] Train samples: {train_mask.sum()}\")\n",
    "    print(f\"[INFO] Test samples: {test_mask.sum()}\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "    # Save checkpoint - but device will be re-detected next time\n",
    "    save_checkpoint('training_setup', {\n",
    "        'train_mask': train_mask,\n",
    "        'test_mask': test_mask,\n",
    "        'device': device  # This will be ignored on reload\n",
    "    }, \"Training setup checkpoint saved\")\n",
    "\n",
    "# Initialize model and training components\n",
    "print(f\"[INFO] Initializing model on device: {device}\")\n",
    "model = ImprovedGNN(num_features=data.x.shape[1], hidden_dim=128, dropout=0.3).to(device)\n",
    "\n",
    "# IMPORTANT: Move data to device for NeighborLoader\n",
    "print(\"[INFO] Moving graph data to device...\")\n",
    "data = data.to(device)\n",
    "\n",
    "# Weighted loss for imbalanced classes\n",
    "class_weights_tensor = torch.tensor([class_weight_dict[0], class_weight_dict[1]], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=20)\n",
    "\n",
    "# STEP 7: CREATE NEIGHBORLOADER FOR MEMORY-EFFICIENT TRAINING\n",
    "print(\"[INFO] Creating NeighborLoader for memory-efficient training...\")\n",
    "\n",
    "# Create training and test loaders with neighbor sampling\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[15, 10, 5],  # Sample 15 neighbors for layer 1, 10 for layer 2, 5 for layer 3\n",
    "    batch_size=1024,  # Process 1024 nodes at a time\n",
    "    input_nodes=train_mask,\n",
    "    shuffle=True,\n",
    "    num_workers= 4  # Set to 0 for Colab compatibility\n",
    ")\n",
    "\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[15, 10, 5],\n",
    "    batch_size=1024,\n",
    "    input_nodes=test_mask,\n",
    "    shuffle=False,\n",
    "    num_workers= 4\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Training batches: {len(train_loader)}\")\n",
    "print(f\"[INFO] Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Check for training resume\n",
    "training_checkpoint_path = os.path.join(CHECKPOINT_DIR, 'training_checkpoint.pth')\n",
    "start_epoch = 0\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "test_f1_scores = []\n",
    "\n",
    "if os.path.exists(training_checkpoint_path):\n",
    "    print(\"[RESUME] Loading training checkpoint...\")\n",
    "    checkpoint = torch.load(training_checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    patience_counter = checkpoint['patience_counter']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    test_f1_scores = checkpoint['test_f1_scores']\n",
    "    print(f\"[SUCCESS] Resumed training from epoch {start_epoch}, best F1: {best_f1:.4f}\")\n",
    "\n",
    "print(f\"[INFO] Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"[INFO] Model device: {next(model.parameters()).device}\")\n",
    "print(f\"[INFO] Data device: {data.x.device}\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    global start_epoch, best_f1, patience_counter\n",
    "    \n",
    "    # STEP 8: Training loop with NeighborLoader (MEMORY EFFICIENT)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸš€ TRAINING WITH NEIGHBORLOADER - MEMORY EFFICIENT ON CUDA\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    patience = 50\n",
    "    max_epochs = 500\n",
    "\n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "        # Training with mini-batches\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Batch should already be on the correct device since data is on device\n",
    "            # But let's be explicit\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass on batch\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            loss = criterion(out, batch.y)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Evaluation every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            all_probs = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    out = model(batch.x, batch.edge_index)\n",
    "                    pred_proba = F.softmax(out, dim=1)[:, 1]\n",
    "                    pred = out.argmax(dim=1)\n",
    "\n",
    "                    all_preds.extend(pred.cpu().numpy())\n",
    "                    all_labels.extend(batch.y.cpu().numpy())\n",
    "                    all_probs.extend(pred_proba.cpu().numpy())\n",
    "\n",
    "            # Convert to numpy arrays\n",
    "            y_test_np = np.array(all_labels)\n",
    "            y_pred_np = np.array(all_preds)\n",
    "            y_pred_proba_np = np.array(all_probs)\n",
    "\n",
    "            # Find optimal threshold\n",
    "            if len(np.unique(y_test_np)) > 1:\n",
    "                precision, recall, thresholds = precision_recall_curve(y_test_np, y_pred_proba_np)\n",
    "                f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "                best_threshold_idx = np.argmax(f1_scores)\n",
    "                best_threshold = thresholds[best_threshold_idx] if len(thresholds) > 0 else 0.5\n",
    "\n",
    "                y_pred_threshold = (y_pred_proba_np >= best_threshold).astype(int)\n",
    "                test_f1 = f1_score(y_test_np, y_pred_threshold)\n",
    "            else:\n",
    "                test_f1 = 0\n",
    "                best_threshold = 0.5\n",
    "\n",
    "            test_f1_scores.append(test_f1)\n",
    "\n",
    "            # Calculate training accuracy (sample from training loader)\n",
    "            model.eval()\n",
    "            train_preds_agg = []\n",
    "            train_labels_agg = []\n",
    "            train_batches_checked = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in train_loader:\n",
    "                    if train_batches_checked >= 5:  # Sample first 5 batches for speed\n",
    "                        break\n",
    "                    batch = batch.to(device)\n",
    "                    out = model(batch.x, batch.edge_index)\n",
    "                    pred = out.argmax(dim=1)\n",
    "                    train_preds_agg.extend(pred.cpu().numpy())\n",
    "                    train_labels_agg.extend(batch.y.cpu().numpy())\n",
    "                    train_batches_checked += 1\n",
    "\n",
    "            train_acc = np.mean(np.array(train_preds_agg) == np.array(train_labels_agg))\n",
    "            test_acc = np.mean(y_pred_np == y_test_np)\n",
    "\n",
    "            print(f'Epoch {epoch:03d}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Threshold: {best_threshold:.3f}')\n",
    "\n",
    "            # Early stopping based on F1 score\n",
    "            if test_f1 > best_f1:\n",
    "                best_f1 = test_f1\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_gnn_model.pth'))\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Save training checkpoint every 20 epochs\n",
    "            if epoch % 20 == 0:\n",
    "                save_training_checkpoint(epoch, model, optimizer, scheduler, best_f1, patience_counter, train_losses, test_f1_scores)\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "            scheduler.step(test_f1)\n",
    "\n",
    "    print(f\"\\nâœ… TRAINING COMPLETED ON: {device}\")\n",
    "    print(f\"ðŸ“Š Final device check - Model: {next(model.parameters()).device}\")\n",
    "\n",
    "    # Final checkpoint save\n",
    "    save_training_checkpoint(epoch, model, optimizer, scheduler, best_f1, patience_counter, train_losses, test_f1_scores)\n",
    "\n",
    "    # Load best model for final evaluation\n",
    "    best_model_path = os.path.join(CHECKPOINT_DIR, 'best_gnn_model.pth')\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "    # STEP 9: Final evaluation with NeighborLoader (using aggregated results from test loader)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š FINAL EVALUATION WITH NEIGHBORLOADER (Aggregated Test Batches)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            pred_proba = F.softmax(out, dim=1)[:, 1]\n",
    "            pred = out.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(batch.y.cpu().numpy())\n",
    "            all_probs.extend(pred_proba.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_test = np.array(all_labels)\n",
    "    y_pred_proba_test = np.array(all_probs)\n",
    "\n",
    "    # Find optimal threshold\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_test)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    best_threshold_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[best_threshold_idx] if len(thresholds) > 0 else 0.5 # Handle case with no positive predictions\n",
    "\n",
    "    y_pred_test = (y_pred_proba_test >= optimal_threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    # Note: Accuracy is calculated on aggregated batch predictions, not the true overall test set.\n",
    "    test_acc = np.mean(np.array(all_preds) == y_test)\n",
    "    # AUC and F1 are correctly calculated on aggregated test predictions and probabilities\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba_test) if len(np.unique(y_test)) > 1 else 0.0 # Handle case with no positive labels in test set\n",
    "    final_f1 = f1_score(y_test, y_pred_test) if len(np.unique(y_test)) > 1 else 0.0\n",
    "\n",
    "\n",
    "    print(f\"Test Accuracy (Aggregated): {test_acc:.4f}\")\n",
    "    print(f\"AUC Score (Aggregated): {auc_score:.4f}\")\n",
    "    print(f\"F1 Score (Aggregated): {final_f1:.4f}\")\n",
    "    print(f\"Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "\n",
    "    print(\"\\nClassification Report (Aggregated Test Batches):\")\n",
    "    # Ensure target names match unique labels present in y_test\n",
    "    target_names = ['Licit', 'Illicit']\n",
    "    report = classification_report(y_test, y_pred_test, target_names=target_names, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Aggregated Test Batches):\")\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    print(\"   Predicted:  Licit  Illicit\")\n",
    "    print(f\"   Licit:      {cm[0,0]:5d}     {cm[0,1]:4d}\")\n",
    "    print(f\"   Illicit:      {cm[1,0]:3d}     {cm[1,1]:4d}\")\n",
    "\n",
    "\n",
    "    # STEP 10: Save the final model\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ’¾ SAVING FINAL MODEL WITH NEIGHBORLOADER SUPPORT\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    model_data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'scaler': scaler,\n",
    "        'addr_to_idx': addr_to_idx,\n",
    "        'feature_cols': feature_cols,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'class_weights': class_weight_dict,\n",
    "        'model_config': {\n",
    "            'num_features': data.x.shape[1],\n",
    "            'hidden_dim': 128,\n",
    "            'num_classes': 2,\n",
    "            'dropout': 0.3\n",
    "        },\n",
    "        'training_history': {\n",
    "            'train_losses': train_losses,\n",
    "            'test_f1_scores': test_f1_scores,\n",
    "            'final_metrics': {\n",
    "                'test_accuracy': test_acc, # Note: Aggregated accuracy\n",
    "                'auc_score': auc_score,\n",
    "                'f1_score': final_f1,\n",
    "                'optimal_threshold': optimal_threshold\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    torch.save(model_data, 'improved_gnn_ransomware_model_neighborloader.pth')\n",
    "    print(\"[SUCCESS] Final NeighborLoader model saved!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ensure num_workers is set to > 0 in STEP 7\n",
    "    # For example: num_workers=4\n",
    "    main()\n",
    "# Clean up checkpoints (optional)\n",
    "print(\"\\nðŸ§¹ CLEANUP OPTIONS:\")\n",
    "print(\"To clean up checkpoints and save space, run:\")\n",
    "print(\"  import shutil; shutil.rmtree('/content/checkpoints')\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ NEIGHBORLOADER TRAINING ADOPTED:\")\n",
    "print(f\"  âœ… Re-enabled NeighborLoader for memory efficiency\")\n",
    "print(f\"  âœ… Training and evaluation using mini-batches\")\n",
    "print(f\"  âœ… Requires torch-sparse (installed)\")\n",
    "print(f\"  âš ï¸ Evaluation metrics are aggregated over test batches, not the entire test set at once.\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Training Attempted:\")\n",
    "print(f\"  - Training with NeighborLoader should now proceed if torch-sparse is correctly installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a26a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  import torch_geometric.typing\n",
      "C:\\Users\\CATURWARGA COMPUTER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ IMPROVED GNN RANSOMWARE DETECTION MODEL WITH NEIGHBORLOADER\n",
      "======================================================================\n",
      "[RESUME] Loading data from checkpoint...\n",
      "[SUCCESS] Resumed with 1268260 wallet records and 2868964 edges\n",
      "[RESUME] Loading feature engineering from checkpoint...\n",
      "[SUCCESS] Resumed with 68 columns\n",
      "[RESUME] Loading cleaned data from checkpoint...\n",
      "[SUCCESS] Resumed with 367472 cleaned addresses\n",
      "[INFO] Class distribution: {0: 338871, 1: 28601}\n",
      "[INFO] Class weights: {0: np.float64(0.5422004243502689), 1: np.float64(6.424111045068354)}\n",
      "[RESUME] Loading graph from checkpoint...\n",
      "[SUCCESS] Resumed with graph: 265354 nodes, 2236444 edges\n",
      "[INFO] Forcing CUDA device detection...\n",
      "[INFO] CUDA Available: True\n",
      "[INFO] Device: cuda\n",
      "[INFO] CUDA Device: NVIDIA GeForce RTX 2060 with Max-Q Design\n",
      "[INFO] CUDA Memory: 6.44 GB\n",
      "[RESUME] Loading training setup from checkpoint...\n",
      "[SUCCESS] Resumed training setup\n",
      "[INFO] Train samples: 212283\n",
      "[INFO] Test samples: 53071\n",
      "[INFO] Device: cuda\n",
      "[INFO] Initializing model on device: cuda\n",
      "[INFO] Moving graph data to device...\n",
      "[INFO] Creating NeighborLoader for memory-efficient training...\n",
      "[INFO] Training batches: 208\n",
      "[INFO] Test batches: 52\n",
      "[RESUME] Loading training checkpoint...\n",
      "[SUCCESS] Resumed training from epoch 341, best F1: 0.7304\n",
      "[INFO] Model parameters: 398434\n",
      "[INFO] Model device: cuda:0\n",
      "[INFO] Data device: cuda:0\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ TRAINING WITH NEIGHBORLOADER - MEMORY EFFICIENT ON CUDA\n",
      "======================================================================\n",
      "Epoch 350, Loss: 0.1546, Train Acc: 0.8961, Test Acc: 0.8959, F1: 0.7271, Threshold: 0.927\n",
      "Epoch 360, Loss: 0.1535, Train Acc: 0.9019, Test Acc: 0.8963, F1: 0.7117, Threshold: 0.939\n",
      "ðŸ’¾ Training checkpoint saved at epoch 360\n",
      "Epoch 370, Loss: 0.1525, Train Acc: 0.8963, Test Acc: 0.8952, F1: 0.7442, Threshold: 0.919\n",
      "Epoch 380, Loss: 0.1532, Train Acc: 0.8992, Test Acc: 0.8974, F1: 0.7105, Threshold: 0.946\n",
      "ðŸ’¾ Training checkpoint saved at epoch 380\n",
      "Epoch 390, Loss: 0.1504, Train Acc: 0.9057, Test Acc: 0.8974, F1: 0.7349, Threshold: 0.937\n",
      "Epoch 400, Loss: 0.1517, Train Acc: 0.9045, Test Acc: 0.8956, F1: 0.7228, Threshold: 0.956\n",
      "ðŸ’¾ Training checkpoint saved at epoch 400\n",
      "Epoch 410, Loss: 0.1489, Train Acc: 0.9003, Test Acc: 0.8945, F1: 0.7409, Threshold: 0.947\n",
      "Epoch 420, Loss: 0.1532, Train Acc: 0.8951, Test Acc: 0.8935, F1: 0.7112, Threshold: 0.949\n",
      "ðŸ’¾ Training checkpoint saved at epoch 420\n",
      "Epoch 430, Loss: 0.1513, Train Acc: 0.8984, Test Acc: 0.8969, F1: 0.7247, Threshold: 0.943\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 497\u001b[0m\n\u001b[0;32m    494\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# Forward pass on batch\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, batch\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m    500\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 331\u001b[0m, in \u001b[0;36mImprovedGNN.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m    328\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(h1)\n\u001b[0;32m    329\u001b[0m h1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(h1, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m--> 331\u001b[0m h2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    332\u001b[0m h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(h2)\n\u001b[0;32m    333\u001b[0m h2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(h2, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "# Re-import NeighborLoader for mini-batch training\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# CHECKPOINT SYSTEM\n",
    "CHECKPOINT_DIR = 'Ransomware_Checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(checkpoint_name, data, message=\"Checkpoint saved\"):\n",
    "    \"\"\"Save checkpoint data\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"âœ… {message}: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_name):\n",
    "    \"\"\"Load checkpoint data\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def checkpoint_exists(checkpoint_name):\n",
    "    \"\"\"Check if checkpoint exists\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    return os.path.exists(checkpoint_path)\n",
    "\n",
    "# Updated title for clarity\n",
    "print(\"ðŸš€ IMPROVED GNN RANSOMWARE DETECTION MODEL WITH NEIGHBORLOADER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STEP 1: Check for data loading checkpoint\n",
    "if checkpoint_exists('data_loaded'):\n",
    "    print(\"[RESUME] Loading data from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('data_loaded')\n",
    "    wallets_df = checkpoint_data['wallets_df']\n",
    "    edges_df = checkpoint_data['edges_df']\n",
    "    print(f\"[SUCCESS] Resumed with {len(wallets_df)} wallet records and {len(edges_df)} edges\")\n",
    "else:\n",
    "    print(\"[INFO] Loading wallet features and classes...\")\n",
    "    wallets_df = pd.read_csv('Elliptic++ Dataset/wallets_features_classes_combined.csv')\n",
    "    print(f\"[SUCCESS] Loaded {len(wallets_df)} wallet records\")\n",
    "\n",
    "    print(\"[INFO] Loading address-to-address edges...\")\n",
    "    edges_df = pd.read_csv('Elliptic++ Dataset/AddrAddr_edgelist.csv')\n",
    "    print(f\"[SUCCESS] Loaded {len(edges_df)} edges\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('data_loaded', {\n",
    "        'wallets_df': wallets_df,\n",
    "        'edges_df': edges_df\n",
    "    }, \"Data loading checkpoint saved\")\n",
    "\n",
    "# STEP 2: Check for feature engineering checkpoint\n",
    "def create_enhanced_pattern_features(df):\n",
    "    \"\"\"Create the EXACT same enhanced features as XGBoost model\"\"\"\n",
    "\n",
    "    # Fill missing values first (same as XGBoost)\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "\n",
    "    # 1. Partner Transaction Ratio (connectivity)\n",
    "    df['partner_transaction_ratio'] = (\n",
    "        df.get('transacted_w_address_total', 0) /\n",
    "        (df.get('total_txs', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 2. Activity Density (txs per block)\n",
    "    df['activity_density'] = (\n",
    "        df.get('total_txs', 0) /\n",
    "        (df.get('lifetime_in_blocks', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 3. Transaction Size Variance (volatility)\n",
    "    df['transaction_size_variance'] = (\n",
    "        df.get('btc_transacted_max', 0) - df.get('btc_transacted_min', 0)\n",
    "    ) / (df.get('btc_transacted_mean', 1) + 1e-8)\n",
    "\n",
    "    # 4. Flow Imbalance (money laundering indicator)\n",
    "    df['flow_imbalance'] = (\n",
    "        (df.get('btc_sent_total', 0) - df.get('btc_received_total', 0)) /\n",
    "        (df.get('btc_transacted_total', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 5. Temporal Spread (time pattern)\n",
    "    df['temporal_spread'] = (\n",
    "        df.get('last_block_appeared_in', 0) - df.get('first_block_appeared_in', 0)\n",
    "    ) / (df.get('num_timesteps_appeared_in', 1) + 1e-8)\n",
    "\n",
    "    # 6. Fee Percentile (urgency indicator)\n",
    "    df['fee_percentile'] = (\n",
    "        df.get('fees_total', 0) /\n",
    "        (df.get('btc_transacted_total', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 7. Interaction Intensity (network centrality)\n",
    "    df['interaction_intensity'] = (\n",
    "        df.get('num_addr_transacted_multiple', 0) /\n",
    "        (df.get('transacted_w_address_total', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 8. Value Per Transaction (transaction size)\n",
    "    df['value_per_transaction'] = (\n",
    "        df.get('btc_transacted_total', 0) /\n",
    "        (df.get('total_txs', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 9. RANSOMWARE-SPECIFIC: Burst Activity (rapid txs)\n",
    "    df['burst_activity'] = (\n",
    "        df.get('total_txs', 0) * df.get('activity_density', 0)\n",
    "    )\n",
    "\n",
    "    # 10. RANSOMWARE-SPECIFIC: Mixing Intensity (obfuscation)\n",
    "    df['mixing_intensity'] = (\n",
    "        df.get('partner_transaction_ratio', 0) * df.get('interaction_intensity', 0)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "if checkpoint_exists('features_engineered'):\n",
    "    print(\"[RESUME] Loading feature engineering from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('features_engineered')\n",
    "    wallets_df = checkpoint_data['wallets_df']\n",
    "    print(f\"[SUCCESS] Resumed with {len(wallets_df.columns)} columns\")\n",
    "else:\n",
    "    print(\"[INFO] Creating enhanced features...\")\n",
    "    wallets_df = create_enhanced_pattern_features(wallets_df)\n",
    "    print(f\"[SUCCESS] Enhanced features created. Now have {len(wallets_df.columns)} columns\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('features_engineered', {\n",
    "        'wallets_df': wallets_df\n",
    "    }, \"Feature engineering checkpoint saved\")\n",
    "\n",
    "# STEP 3: Check for data cleaning checkpoint\n",
    "if checkpoint_exists('data_cleaned'):\n",
    "    print(\"[RESUME] Loading cleaned data from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('data_cleaned')\n",
    "    wallets_clean = checkpoint_data['wallets_clean']\n",
    "    class_weight_dict = checkpoint_data['class_weight_dict']\n",
    "    print(f\"[SUCCESS] Resumed with {len(wallets_clean)} cleaned addresses\")\n",
    "    print(f\"[INFO] Class distribution: {wallets_clean['class'].value_counts().to_dict()}\")\n",
    "    print(f\"[INFO] Class weights: {class_weight_dict}\")\n",
    "else:\n",
    "    print(\"[INFO] Cleaning data with same logic as XGBoost...\")\n",
    "\n",
    "    # Keep only labeled addresses (1=Illicit, 2=Licit)\n",
    "    wallets_clean = wallets_df[wallets_df['class'].isin([1, 2])].copy()\n",
    "    print(f\"[INFO] Labeled addresses: {len(wallets_clean)}\")\n",
    "\n",
    "    # Remap classes: 1->1 (Illicit), 2->0 (Licit) - SAME AS XGBOOST\n",
    "    wallets_clean['class'] = wallets_clean['class'].map({1: 1, 2: 0})\n",
    "    print(f\"[INFO] Class distribution: {wallets_clean['class'].value_counts().to_dict()}\")\n",
    "\n",
    "    # Calculate class weights (same as XGBoost)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(wallets_clean['class']), y=wallets_clean['class'])\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    print(f\"[INFO] Class weights: {class_weight_dict}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('data_cleaned', {\n",
    "        'wallets_clean': wallets_clean,\n",
    "        'class_weight_dict': class_weight_dict\n",
    "    }, \"Data cleaning checkpoint saved\")\n",
    "\n",
    "# STEP 4: Check for graph construction checkpoint\n",
    "if checkpoint_exists('graph_built'):\n",
    "    print(\"[RESUME] Loading graph from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('graph_built')\n",
    "    data = checkpoint_data['data']\n",
    "    addr_to_idx = checkpoint_data['addr_to_idx']\n",
    "    feature_cols = checkpoint_data['feature_cols']\n",
    "    scaler = checkpoint_data['scaler']\n",
    "    X_scaled = checkpoint_data['X_scaled']\n",
    "    print(f\"[SUCCESS] Resumed with graph: {data.x.shape[0]} nodes, {data.edge_index.shape[1]} edges\")\n",
    "else:\n",
    "    print(\"[INFO] Building improved graph structure...\")\n",
    "    print(\"This is the slow part - creating address mappings and features...\")\n",
    "\n",
    "    # Create address to index mapping\n",
    "    unique_addresses = wallets_clean['address'].unique()\n",
    "    addr_to_idx = {addr: idx for idx, addr in enumerate(unique_addresses)}\n",
    "\n",
    "    # Prepare features and labels\n",
    "    exclude_cols = ['address', 'Time step', 'class']\n",
    "    feature_cols = [col for col in wallets_clean.columns if col not in exclude_cols]\n",
    "\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    address_list = []\n",
    "\n",
    "    print(f\"[INFO] Processing {len(unique_addresses)} unique addresses...\")\n",
    "    for i, addr in enumerate(unique_addresses):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"  Progress: {i}/{len(unique_addresses)} ({i/len(unique_addresses)*100:.1f}%)\")\n",
    "\n",
    "        addr_data = wallets_clean[wallets_clean['address'] == addr].iloc[0]\n",
    "        features_list.append(addr_data[feature_cols].values)\n",
    "        labels_list.append(addr_data['class'])\n",
    "        address_list.append(addr)\n",
    "\n",
    "    X = np.array(features_list)\n",
    "    y = np.array(labels_list)\n",
    "\n",
    "    print(f\"[INFO] Feature matrix shape: {X.shape}\")\n",
    "    print(f\"[INFO] Labels shape: {y.shape}\")\n",
    "\n",
    "    # Create edge list with weights\n",
    "    print(\"[INFO] Creating weighted edges...\")\n",
    "    edge_list = []\n",
    "    edge_weights = []\n",
    "\n",
    "    print(f\"[INFO] Processing {len(edges_df)} edges...\")\n",
    "    for i, (_, row) in enumerate(edges_df.iterrows()):\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"  Edge progress: {i}/{len(edges_df)} ({i/len(edges_df)*100:.1f}%)\")\n",
    "\n",
    "        input_addr = row['input_address']\n",
    "        output_addr = row['output_address']\n",
    "\n",
    "        if input_addr in addr_to_idx and output_addr in addr_to_idx:\n",
    "            input_idx = addr_to_idx[input_addr]\n",
    "            output_idx = addr_to_idx[output_addr]\n",
    "\n",
    "            # Add both directions for undirected graph\n",
    "            edge_list.extend([[input_idx, output_idx], [output_idx, input_idx]])\n",
    "\n",
    "            # Use transaction amount as edge weight if available\n",
    "            weight = row.get('value', 1.0) if 'value' in row else 1.0\n",
    "            edge_weights.extend([weight, weight])\n",
    "\n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "    print(f\"[INFO] Graph edges: {edge_index.shape[1]} edges (bidirectional)\")\n",
    "\n",
    "    # Scale features\n",
    "    print(\"[INFO] Scaling features with RobustScaler...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.tensor(X_scaled, dtype=torch.float)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    print(f\"[INFO] Final graph structure:\")\n",
    "    print(f\"  - Nodes: {x.shape[0]}\")\n",
    "    print(f\"  - Node features: {x.shape[1]}\")\n",
    "    f\"  - Edges: {edge_index.shape[1]}\"\n",
    "    print(f\"  - Classes: Licit={sum(y==0)}, Illicit={sum(y==1)}\")\n",
    "\n",
    "    # Create PyTorch Geometric data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_weights, y=y)\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('graph_built', {\n",
    "        'data': data,\n",
    "        'addr_to_idx': addr_to_idx,\n",
    "        'feature_cols': feature_cols,\n",
    "        'scaler': scaler,\n",
    "        'X_scaled': X_scaled\n",
    "    }, \"Graph construction checkpoint saved\")\n",
    "\n",
    "# STEP 5: Model Definition (Updated for batch processing and torch.nn.BatchNorm1d)\n",
    "class ImprovedGNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super(ImprovedGNN, self).__init__()\n",
    "\n",
    "        # Graph attention layers for better feature learning\n",
    "        self.gat1 = GATConv(num_features, hidden_dim, heads=4, dropout=dropout)\n",
    "        # Use standard torch.nn.BatchNorm1d\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim * 4)\n",
    "\n",
    "        self.gat2 = GATConv(hidden_dim * 4, hidden_dim, heads=4, dropout=dropout)\n",
    "        # Use standard torch.nn.BatchNorm1d\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim * 4)\n",
    "\n",
    "        self.gat3 = GATConv(hidden_dim * 4, hidden_dim // 2, heads=2, dropout=dropout)\n",
    "        # Use standard torch.nn.BatchNorm1d\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim) # output shape after gat3 is hidden_dim // 2 * 2 = hidden_dim\n",
    "\n",
    "        # Feature-only path (like XGBoost)\n",
    "        self.feature_path = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Combined classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim // 2),  # Graph + Feature paths\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Graph path\n",
    "        h1 = F.elu(self.gat1(x, edge_index))\n",
    "        h1 = self.bn1(h1)\n",
    "        h1 = F.dropout(h1, p=self.dropout, training=self.training)\n",
    "\n",
    "        h2 = F.elu(self.gat2(h1, edge_index))\n",
    "        h2 = self.bn2(h2)\n",
    "        h2 = F.dropout(h2, p=self.dropout, training=self.training)\n",
    "\n",
    "        h3 = F.elu(self.gat3(h2, edge_index))\n",
    "        h3 = self.bn3(h3)\n",
    "        graph_features = F.dropout(h3, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Feature-only path (mimics XGBoost)\n",
    "        feature_only = self.feature_path(x)\n",
    "\n",
    "        # Combine both paths\n",
    "        combined = torch.cat([graph_features, feature_only], dim=1)\n",
    "\n",
    "        # Classification\n",
    "        out = self.classifier(combined)\n",
    "\n",
    "        return out\n",
    "\n",
    "# STEP 6: Training setup with NeighborLoader - FIXED CUDA DETECTION\n",
    "def save_training_checkpoint(epoch, model, optimizer, scheduler, best_f1, patience_counter, train_losses, test_f1_scores):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    training_state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'best_f1': best_f1,\n",
    "        'patience_counter': patience_counter,\n",
    "        'train_losses': train_losses,\n",
    "        'test_f1_scores': test_f1_scores\n",
    "    }\n",
    "    torch.save(training_state, os.path.join(CHECKPOINT_DIR, 'training_checkpoint.pth'))\n",
    "    print(f\"ðŸ’¾ Training checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "# FORCE CUDA DETECTION - Don't load device from checkpoint\n",
    "print(\"[INFO] Forcing CUDA device detection...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[INFO] CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[INFO] CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"[INFO] CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Check for training setup checkpoint - but don't load device from it\n",
    "if checkpoint_exists('training_setup'):\n",
    "    print(\"[RESUME] Loading training setup from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('training_setup')\n",
    "    train_mask = checkpoint_data['train_mask']\n",
    "    test_mask = checkpoint_data['test_mask']\n",
    "    # DON'T LOAD DEVICE FROM CHECKPOINT - use current detection\n",
    "    print(f\"[SUCCESS] Resumed training setup\")\n",
    "    print(f\"[INFO] Train samples: {train_mask.sum()}\")\n",
    "    print(f\"[INFO] Test samples: {test_mask.sum()}\")\n",
    "    print(f\"[INFO] Device: {device}\")  # Use current device\n",
    "else:\n",
    "    print(\"[INFO] Creating stratified train-test split...\")\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(data.y)), test_size=0.2, random_state=42,\n",
    "        stratify=data.y.numpy()\n",
    "    )\n",
    "\n",
    "    train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "\n",
    "    print(f\"[INFO] Train samples: {train_mask.sum()}\")\n",
    "    print(f\"[INFO] Test samples: {test_mask.sum()}\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "    # Save checkpoint - but device will be re-detected next time\n",
    "    save_checkpoint('training_setup', {\n",
    "        'train_mask': train_mask,\n",
    "        'test_mask': test_mask,\n",
    "        'device': device  # This will be ignored on reload\n",
    "    }, \"Training setup checkpoint saved\")\n",
    "\n",
    "# Initialize model and training components\n",
    "print(f\"[INFO] Initializing model on device: {device}\")\n",
    "model = ImprovedGNN(num_features=data.x.shape[1], hidden_dim=128, dropout=0.3).to(device)\n",
    "\n",
    "# IMPORTANT: Move data to device for NeighborLoader\n",
    "print(\"[INFO] Moving graph data to device...\")\n",
    "data = data.to(device)\n",
    "\n",
    "# Weighted loss for imbalanced classes\n",
    "class_weights_tensor = torch.tensor([class_weight_dict[0], class_weight_dict[1]], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=20)\n",
    "\n",
    "# STEP 7: CREATE NEIGHBORLOADER FOR MEMORY-EFFICIENT TRAINING\n",
    "print(\"[INFO] Creating NeighborLoader for memory-efficient training...\")\n",
    "\n",
    "# Create training and test loaders with neighbor sampling\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[15, 10, 5],  # Sample 15 neighbors for layer 1, 10 for layer 2, 5 for layer 3\n",
    "    batch_size=1024,  # Process 1024 nodes at a time\n",
    "    input_nodes=train_mask,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 for Colab compatibility\n",
    ")\n",
    "\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[15, 10, 5],\n",
    "    batch_size=1024,\n",
    "    input_nodes=test_mask,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Training batches: {len(train_loader)}\")\n",
    "print(f\"[INFO] Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Check for training resume\n",
    "training_checkpoint_path = os.path.join(CHECKPOINT_DIR, 'training_checkpoint.pth')\n",
    "start_epoch = 0\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "test_f1_scores = []\n",
    "\n",
    "if os.path.exists(training_checkpoint_path):\n",
    "    print(\"[RESUME] Loading training checkpoint...\")\n",
    "    checkpoint = torch.load(training_checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    patience_counter = checkpoint['patience_counter']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    test_f1_scores = checkpoint['test_f1_scores']\n",
    "    print(f\"[SUCCESS] Resumed training from epoch {start_epoch}, best F1: {best_f1:.4f}\")\n",
    "\n",
    "print(f\"[INFO] Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"[INFO] Model device: {next(model.parameters()).device}\")\n",
    "print(f\"[INFO] Data device: {data.x.device}\")\n",
    "\n",
    "# STEP 8: Training loop with NeighborLoader (MEMORY EFFICIENT)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ TRAINING WITH NEIGHBORLOADER - MEMORY EFFICIENT ON CUDA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "patience = 50\n",
    "max_epochs = 500\n",
    "\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    # Training with mini-batches\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Batch should already be on the correct device since data is on device\n",
    "        # But let's be explicit\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass on batch\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Evaluation every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.x, batch.edge_index)\n",
    "                pred_proba = F.softmax(out, dim=1)[:, 1]\n",
    "                pred = out.argmax(dim=1)\n",
    "\n",
    "                all_preds.extend(pred.cpu().numpy())\n",
    "                all_labels.extend(batch.y.cpu().numpy())\n",
    "                all_probs.extend(pred_proba.cpu().numpy())\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        y_test_np = np.array(all_labels)\n",
    "        y_pred_np = np.array(all_preds)\n",
    "        y_pred_proba_np = np.array(all_probs)\n",
    "\n",
    "        # Find optimal threshold\n",
    "        if len(np.unique(y_test_np)) > 1:\n",
    "            precision, recall, thresholds = precision_recall_curve(y_test_np, y_pred_proba_np)\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "            best_threshold_idx = np.argmax(f1_scores)\n",
    "            best_threshold = thresholds[best_threshold_idx] if len(thresholds) > 0 else 0.5\n",
    "\n",
    "            y_pred_threshold = (y_pred_proba_np >= best_threshold).astype(int)\n",
    "            test_f1 = f1_score(y_test_np, y_pred_threshold)\n",
    "        else:\n",
    "            test_f1 = 0\n",
    "            best_threshold = 0.5\n",
    "\n",
    "        test_f1_scores.append(test_f1)\n",
    "\n",
    "        # Calculate training accuracy (sample from training loader)\n",
    "        model.eval()\n",
    "        train_preds_agg = []\n",
    "        train_labels_agg = []\n",
    "        train_batches_checked = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in train_loader:\n",
    "                if train_batches_checked >= 5:  # Sample first 5 batches for speed\n",
    "                    break\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.x, batch.edge_index)\n",
    "                pred = out.argmax(dim=1)\n",
    "                train_preds_agg.extend(pred.cpu().numpy())\n",
    "                train_labels_agg.extend(batch.y.cpu().numpy())\n",
    "                train_batches_checked += 1\n",
    "\n",
    "        train_acc = np.mean(np.array(train_preds_agg) == np.array(train_labels_agg))\n",
    "        test_acc = np.mean(y_pred_np == y_test_np)\n",
    "\n",
    "        print(f'Epoch {epoch:03d}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Threshold: {best_threshold:.3f}')\n",
    "\n",
    "        # Early stopping based on F1 score\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_gnn_model.pth'))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Save training checkpoint every 20 epochs\n",
    "        if epoch % 20 == 0:\n",
    "            save_training_checkpoint(epoch, model, optimizer, scheduler, best_f1, patience_counter, train_losses, test_f1_scores)\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(test_f1)\n",
    "\n",
    "print(f\"\\nâœ… TRAINING COMPLETED ON: {device}\")\n",
    "print(f\"ðŸ“Š Final device check - Model: {next(model.parameters()).device}\")\n",
    "\n",
    "# Final checkpoint save\n",
    "save_training_checkpoint(epoch, model, optimizer, scheduler, best_f1, patience_counter, train_losses, test_f1_scores)\n",
    "\n",
    "# Load best model for final evaluation\n",
    "best_model_path = os.path.join(CHECKPOINT_DIR, 'best_gnn_model.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "# STEP 9: Final evaluation with NeighborLoader (using aggregated results from test loader)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š FINAL EVALUATION WITH NEIGHBORLOADER (Aggregated Test Batches)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        pred_proba = F.softmax(out, dim=1)[:, 1]\n",
    "        pred = out.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(batch.y.cpu().numpy())\n",
    "        all_probs.extend(pred_proba.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_test = np.array(all_labels)\n",
    "y_pred_proba_test = np.array(all_probs)\n",
    "\n",
    "# Find optimal threshold\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_test)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[best_threshold_idx] if len(thresholds) > 0 else 0.5 # Handle case with no positive predictions\n",
    "\n",
    "y_pred_test = (y_pred_proba_test >= optimal_threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "# Note: Accuracy is calculated on aggregated batch predictions, not the true overall test set.\n",
    "test_acc = np.mean(np.array(all_preds) == y_test)\n",
    "# AUC and F1 are correctly calculated on aggregated test predictions and probabilities\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba_test) if len(np.unique(y_test)) > 1 else 0.0 # Handle case with no positive labels in test set\n",
    "final_f1 = f1_score(y_test, y_pred_test) if len(np.unique(y_test)) > 1 else 0.0\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy (Aggregated): {test_acc:.4f}\")\n",
    "print(f\"AUC Score (Aggregated): {auc_score:.4f}\")\n",
    "print(f\"F1 Score (Aggregated): {final_f1:.4f}\")\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Aggregated Test Batches):\")\n",
    "# Ensure target names match unique labels present in y_test\n",
    "target_names = ['Licit', 'Illicit']\n",
    "report = classification_report(y_test, y_pred_test, target_names=target_names, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "\n",
    "print(\"\\nConfusion Matrix (Aggregated Test Batches):\")\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"   Predicted:  Licit  Illicit\")\n",
    "print(f\"   Licit:      {cm[0,0]:5d}     {cm[0,1]:4d}\")\n",
    "print(f\"   Illicit:      {cm[1,0]:3d}     {cm[1,1]:4d}\")\n",
    "\n",
    "\n",
    "# STEP 10: Save the final model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ’¾ SAVING FINAL MODEL WITH NEIGHBORLOADER SUPPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_data = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'addr_to_idx': addr_to_idx,\n",
    "    'feature_cols': feature_cols,\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'class_weights': class_weight_dict,\n",
    "    'model_config': {\n",
    "        'num_features': data.x.shape[1],\n",
    "        'hidden_dim': 128,\n",
    "        'num_classes': 2,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'training_history': {\n",
    "        'train_losses': train_losses,\n",
    "        'test_f1_scores': test_f1_scores,\n",
    "        'final_metrics': {\n",
    "            'test_accuracy': test_acc, # Note: Aggregated accuracy\n",
    "            'auc_score': auc_score,\n",
    "            'f1_score': final_f1,\n",
    "            'optimal_threshold': optimal_threshold\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(model_data, 'improved_gnn_ransomware_model_neighborloader.pth')\n",
    "print(\"[SUCCESS] Final NeighborLoader model saved!\")\n",
    "\n",
    "# Clean up checkpoints (optional)\n",
    "print(\"\\nðŸ§¹ CLEANUP OPTIONS:\")\n",
    "print(\"To clean up checkpoints and save space, run:\")\n",
    "print(\"  import shutil; shutil.rmtree('/content/checkpoints')\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ NEIGHBORLOADER TRAINING ADOPTED:\")\n",
    "print(f\"  âœ… Re-enabled NeighborLoader for memory efficiency\")\n",
    "print(f\"  âœ… Training and evaluation using mini-batches\")\n",
    "print(f\"  âœ… Requires torch-sparse (installed)\")\n",
    "print(f\"  âš ï¸ Evaluation metrics are aggregated over test batches, not the entire test set at once.\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Training Attempted:\")\n",
    "print(f\"  - Training with NeighborLoader should now proceed if torch-sparse is correctly installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b59a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sanity check:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Quick sanity check\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData sanity check:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39my[train_mask]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39my[test_mask]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining class distribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mbincount(data\u001b[38;5;241m.\u001b[39my[train_mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Quick sanity check\n",
    "print(\"Data sanity check:\")\n",
    "print(f\"Training labels: {data.y[train_mask].unique()}\")\n",
    "print(f\"Test labels: {data.y[test_mask].unique()}\")\n",
    "print(f\"Training class distribution: {torch.bincount(data.y[train_mask])}\")\n",
    "print(f\"Test class distribution: {torch.bincount(data.y[test_mask])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7604df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "# Re-import NeighborLoader for mini-batch training\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# CHECKPOINT SYSTEM\n",
    "CHECKPOINT_DIR = 'Ransomware_Checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(checkpoint_name, data, message=\"Checkpoint saved\"):\n",
    "    \"\"\"Save checkpoint data\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"âœ… {message}: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_name):\n",
    "    \"\"\"Load checkpoint data\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def checkpoint_exists(checkpoint_name):\n",
    "    \"\"\"Check if checkpoint exists\"\"\"\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{checkpoint_name}.pkl\")\n",
    "    return os.path.exists(checkpoint_path)\n",
    "\n",
    "# Updated title for clarity\n",
    "print(\"ðŸš€ IMPROVED GNN RANSOMWARE DETECTION MODEL WITH NEIGHBORLOADER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STEP 1: Check for data loading checkpoint\n",
    "if checkpoint_exists('data_loaded'):\n",
    "    print(\"[RESUME] Loading data from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('data_loaded')\n",
    "    wallets_df = checkpoint_data['wallets_df']\n",
    "    edges_df = checkpoint_data['edges_df']\n",
    "    print(f\"[SUCCESS] Resumed with {len(wallets_df)} wallet records and {len(edges_df)} edges\")\n",
    "else:\n",
    "    print(\"[INFO] Loading wallet features and classes...\")\n",
    "    wallets_df = pd.read_csv('Elliptic++ Dataset/wallets_features_classes_combined.csv')\n",
    "    print(f\"[SUCCESS] Loaded {len(wallets_df)} wallet records\")\n",
    "\n",
    "    print(\"[INFO] Loading address-to-address edges...\")\n",
    "    edges_df = pd.read_csv('Elliptic++ Dataset/AddrAddr_edgelist.csv')\n",
    "    print(f\"[SUCCESS] Loaded {len(edges_df)} edges\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('data_loaded', {\n",
    "        'wallets_df': wallets_df,\n",
    "        'edges_df': edges_df\n",
    "    }, \"Data loading checkpoint saved\")\n",
    "\n",
    "# STEP 2: Check for feature engineering checkpoint\n",
    "def create_enhanced_pattern_features(df):\n",
    "    \"\"\"Create the EXACT same enhanced features as XGBoost model\"\"\"\n",
    "\n",
    "    # Fill missing values first (same as XGBoost)\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "\n",
    "    # 1. Partner Transaction Ratio (connectivity)\n",
    "    df['partner_transaction_ratio'] = (\n",
    "        df.get('transacted_w_address_total', 0) /\n",
    "        (df.get('total_txs', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 2. Activity Density (txs per block)\n",
    "    df['activity_density'] = (\n",
    "        df.get('total_txs', 0) /\n",
    "        (df.get('lifetime_in_blocks', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 3. Transaction Size Variance (volatility)\n",
    "    df['transaction_size_variance'] = (\n",
    "        df.get('btc_transacted_max', 0) - df.get('btc_transacted_min', 0)\n",
    "    ) / (df.get('btc_transacted_mean', 1) + 1e-8)\n",
    "\n",
    "    # 4. Flow Imbalance (money laundering indicator)\n",
    "    df['flow_imbalance'] = (\n",
    "        (df.get('btc_sent_total', 0) - df.get('btc_received_total', 0)) /\n",
    "        (df.get('btc_transacted_total', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 5. Temporal Spread (time pattern)\n",
    "    df['temporal_spread'] = (\n",
    "        df.get('last_block_appeared_in', 0) - df.get('first_block_appeared_in', 0)\n",
    "    ) / (df.get('num_timesteps_appeared_in', 1) + 1e-8)\n",
    "\n",
    "    # 6. Fee Percentile (urgency indicator)\n",
    "    df['fee_percentile'] = (\n",
    "        df.get('fees_total', 0) /\n",
    "        (df.get('btc_transacted_total', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 7. Interaction Intensity (network centrality)\n",
    "    df['interaction_intensity'] = (\n",
    "        df.get('num_addr_transacted_multiple', 0) /\n",
    "        (df.get('transacted_w_address_total', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 8. Value Per Transaction (transaction size)\n",
    "    df['value_per_transaction'] = (\n",
    "        df.get('btc_transacted_total', 0) /\n",
    "        (df.get('total_txs', 1) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # 9. RANSOMWARE-SPECIFIC: Burst Activity (rapid txs)\n",
    "    df['burst_activity'] = (\n",
    "        df.get('total_txs', 0) * df.get('activity_density', 0)\n",
    "    )\n",
    "\n",
    "    # 10. RANSOMWARE-SPECIFIC: Mixing Intensity (obfuscation)\n",
    "    df['mixing_intensity'] = (\n",
    "        df.get('partner_transaction_ratio', 0) * df.get('interaction_intensity', 0)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "if checkpoint_exists('features_engineered'):\n",
    "    print(\"[RESUME] Loading feature engineering from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('features_engineered')\n",
    "    wallets_df = checkpoint_data['wallets_df']\n",
    "    print(f\"[SUCCESS] Resumed with {len(wallets_df.columns)} columns\")\n",
    "else:\n",
    "    print(\"[INFO] Creating enhanced features...\")\n",
    "    wallets_df = create_enhanced_pattern_features(wallets_df)\n",
    "    print(f\"[SUCCESS] Enhanced features created. Now have {len(wallets_df.columns)} columns\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('features_engineered', {\n",
    "        'wallets_df': wallets_df\n",
    "    }, \"Feature engineering checkpoint saved\")\n",
    "\n",
    "# STEP 3: Check for data cleaning checkpoint\n",
    "if checkpoint_exists('data_cleaned'):\n",
    "    print(\"[RESUME] Loading cleaned data from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('data_cleaned')\n",
    "    wallets_clean = checkpoint_data['wallets_clean']\n",
    "    class_weight_dict = checkpoint_data['class_weight_dict']\n",
    "    print(f\"[SUCCESS] Resumed with {len(wallets_clean)} cleaned addresses\")\n",
    "    print(f\"[INFO] Class distribution: {wallets_clean['class'].value_counts().to_dict()}\")\n",
    "    print(f\"[INFO] Class weights: {class_weight_dict}\")\n",
    "else:\n",
    "    print(\"[INFO] Cleaning data with same logic as XGBoost...\")\n",
    "\n",
    "    # Keep only labeled addresses (1=Illicit, 2=Licit)\n",
    "    wallets_clean = wallets_df[wallets_df['class'].isin([1, 2])].copy()\n",
    "    print(f\"[INFO] Labeled addresses: {len(wallets_clean)}\")\n",
    "\n",
    "    # Remap classes: 1->1 (Illicit), 2->0 (Licit) - SAME AS XGBOOST\n",
    "    wallets_clean['class'] = wallets_clean['class'].map({1: 1, 2: 0})\n",
    "    print(f\"[INFO] Class distribution: {wallets_clean['class'].value_counts().to_dict()}\")\n",
    "\n",
    "    # Calculate class weights (same as XGBoost)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(wallets_clean['class']), y=wallets_clean['class'])\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "    print(f\"[INFO] Class weights: {class_weight_dict}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('data_cleaned', {\n",
    "        'wallets_clean': wallets_clean,\n",
    "        'class_weight_dict': class_weight_dict\n",
    "    }, \"Data cleaning checkpoint saved\")\n",
    "\n",
    "# STEP 4: Check for graph construction checkpoint\n",
    "if checkpoint_exists('graph_built'):\n",
    "    print(\"[RESUME] Loading graph from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('graph_built')\n",
    "    data = checkpoint_data['data']\n",
    "    addr_to_idx = checkpoint_data['addr_to_idx']\n",
    "    feature_cols = checkpoint_data['feature_cols']\n",
    "    scaler = checkpoint_data['scaler']\n",
    "    X_scaled = checkpoint_data['X_scaled']\n",
    "    print(f\"[SUCCESS] Resumed with graph: {data.x.shape[0]} nodes, {data.edge_index.shape[1]} edges\")\n",
    "else:\n",
    "    print(\"[INFO] Building improved graph structure...\")\n",
    "    print(\"This is the slow part - creating address mappings and features...\")\n",
    "\n",
    "    # Create address to index mapping\n",
    "    unique_addresses = wallets_clean['address'].unique()\n",
    "    addr_to_idx = {addr: idx for idx, addr in enumerate(unique_addresses)}\n",
    "\n",
    "    # Prepare features and labels\n",
    "    exclude_cols = ['address', 'Time step', 'class']\n",
    "    feature_cols = [col for col in wallets_clean.columns if col not in exclude_cols]\n",
    "\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    address_list = []\n",
    "\n",
    "    print(f\"[INFO] Processing {len(unique_addresses)} unique addresses...\")\n",
    "    for i, addr in enumerate(unique_addresses):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"  Progress: {i}/{len(unique_addresses)} ({i/len(unique_addresses)*100:.1f}%)\")\n",
    "\n",
    "        addr_data = wallets_clean[wallets_clean['address'] == addr].iloc[0]\n",
    "        features_list.append(addr_data[feature_cols].values)\n",
    "        labels_list.append(addr_data['class'])\n",
    "        address_list.append(addr)\n",
    "\n",
    "    X = np.array(features_list)\n",
    "    y = np.array(labels_list)\n",
    "\n",
    "    print(f\"[INFO] Feature matrix shape: {X.shape}\")\n",
    "    print(f\"[INFO] Labels shape: {y.shape}\")\n",
    "\n",
    "    # Create edge list with weights\n",
    "    print(\"[INFO] Creating weighted edges...\")\n",
    "    edge_list = []\n",
    "    edge_weights = []\n",
    "\n",
    "    print(f\"[INFO] Processing {len(edges_df)} edges...\")\n",
    "    for i, (_, row) in enumerate(edges_df.iterrows()):\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"  Edge progress: {i}/{len(edges_df)} ({i/len(edges_df)*100:.1f}%)\")\n",
    "\n",
    "        input_addr = row['input_address']\n",
    "        output_addr = row['output_address']\n",
    "\n",
    "        if input_addr in addr_to_idx and output_addr in addr_to_idx:\n",
    "            input_idx = addr_to_idx[input_addr]\n",
    "            output_idx = addr_to_idx[output_addr]\n",
    "\n",
    "            # Add both directions for undirected graph\n",
    "            edge_list.extend([[input_idx, output_idx], [output_idx, input_idx]])\n",
    "\n",
    "            # Use transaction amount as edge weight if available\n",
    "            weight = row.get('value', 1.0) if 'value' in row else 1.0\n",
    "            edge_weights.extend([weight, weight])\n",
    "\n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "    print(f\"[INFO] Graph edges: {edge_index.shape[1]} edges (bidirectional)\")\n",
    "\n",
    "    # Scale features\n",
    "    print(\"[INFO] Scaling features with RobustScaler...\")\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.tensor(X_scaled, dtype=torch.float)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    print(f\"[INFO] Final graph structure:\")\n",
    "    print(f\"  - Nodes: {x.shape[0]}\")\n",
    "    print(f\"  - Node features: {x.shape[1]}\")\n",
    "    f\"  - Edges: {edge_index.shape[1]}\"\n",
    "    print(f\"  - Classes: Licit={sum(y==0)}, Illicit={sum(y==1)}\")\n",
    "\n",
    "    # Create PyTorch Geometric data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_weights, y=y)\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint('graph_built', {\n",
    "        'data': data,\n",
    "        'addr_to_idx': addr_to_idx,\n",
    "        'feature_cols': feature_cols,\n",
    "        'scaler': scaler,\n",
    "        'X_scaled': X_scaled\n",
    "    }, \"Graph construction checkpoint saved\")\n",
    "\n",
    "# STEP 5: Model Definition (Updated for batch processing and torch.nn.BatchNorm1d)\n",
    "class EnhancedGNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, num_classes=2, dropout=0.3):\n",
    "        super(EnhancedGNN, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Graph attention layers with residual connections\n",
    "        self.gat1 = GATConv(num_features, hidden_dim, heads=4, dropout=dropout)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim * 4)\n",
    "        \n",
    "        # Residual connection layer\n",
    "        self.residual1 = nn.Linear(num_features, hidden_dim * 4)\n",
    "        \n",
    "        self.gat2 = GATConv(hidden_dim * 4, hidden_dim, heads=4, dropout=dropout)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim * 4)\n",
    "        \n",
    "        # Residual connection layer\n",
    "        self.residual2 = nn.Linear(hidden_dim * 4, hidden_dim * 4)\n",
    "        \n",
    "        self.gat3 = GATConv(hidden_dim * 4, hidden_dim // 2, heads=2, dropout=dropout)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Enhanced feature path with attention\n",
    "        self.feature_path = nn.Sequential(\n",
    "            nn.Linear(num_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Attention fusion for combining paths\n",
    "        self.attention_fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim // 4, 2),  # 2 attention weights for 2 paths\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Enhanced classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Graph path with residual connections\n",
    "        h1 = F.elu(self.gat1(x, edge_index))\n",
    "        h1 = self.bn1(h1)\n",
    "        h1 = h1 + self.residual1(x)  # Residual connection\n",
    "        h1 = F.dropout(h1, p=self.dropout, training=self.training)\n",
    "        \n",
    "        h2 = F.elu(self.gat2(h1, edge_index))\n",
    "        h2 = self.bn2(h2)\n",
    "        h2 = h2 + self.residual2(h1)  # Residual connection\n",
    "        h2 = F.dropout(h2, p=self.dropout, training=self.training)\n",
    "        \n",
    "        h3 = F.elu(self.gat3(h2, edge_index))\n",
    "        h3 = self.bn3(h3)\n",
    "        graph_features = F.dropout(h3, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Feature-only path\n",
    "        feature_only = self.feature_path(x)\n",
    "        \n",
    "        # Attention-based fusion\n",
    "        combined = torch.cat([graph_features, feature_only], dim=1)\n",
    "        attention_weights = self.attention_fusion(combined)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        weighted_graph = graph_features * attention_weights[:, 0:1]\n",
    "        weighted_features = feature_only * attention_weights[:, 1:2]\n",
    "        \n",
    "        # Final combination\n",
    "        final_features = torch.cat([weighted_graph, weighted_features], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(final_features)\n",
    "        return out\n",
    "\n",
    "# STEP 6: Training setup with NeighborLoader - FIXED CUDA DETECTION\n",
    "def save_training_checkpoint(epoch, model, optimizer, scheduler, best_f1, patience_counter, train_losses, test_f1_scores):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    training_state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'best_f1': best_f1,\n",
    "        'patience_counter': patience_counter,\n",
    "        'train_losses': train_losses,\n",
    "        'test_f1_scores': test_f1_scores\n",
    "    }\n",
    "    torch.save(training_state, os.path.join(CHECKPOINT_DIR, 'training_checkpoint.pth'))\n",
    "    print(f\"ðŸ’¾ Training checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "# FORCE CUDA DETECTION - Don't load device from checkpoint\n",
    "print(\"[INFO] Forcing CUDA device detection...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[INFO] CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[INFO] CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"[INFO] CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Check for training setup checkpoint - but don't load device from it\n",
    "if checkpoint_exists('training_setup'):\n",
    "    print(\"[RESUME] Loading training setup from checkpoint...\")\n",
    "    checkpoint_data = load_checkpoint('training_setup')\n",
    "    train_mask = checkpoint_data['train_mask']\n",
    "    test_mask = checkpoint_data['test_mask']\n",
    "    # DON'T LOAD DEVICE FROM CHECKPOINT - use current detection\n",
    "    print(f\"[SUCCESS] Resumed training setup\")\n",
    "    print(f\"[INFO] Train samples: {train_mask.sum()}\")\n",
    "    print(f\"[INFO] Test samples: {test_mask.sum()}\")\n",
    "    print(f\"[INFO] Device: {device}\")  # Use current device\n",
    "else:\n",
    "    print(\"[INFO] Creating stratified train-test split...\")\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(data.y)), test_size=0.2, random_state=42,\n",
    "        stratify=data.y.numpy()\n",
    "    )\n",
    "\n",
    "    train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "\n",
    "    print(f\"[INFO] Train samples: {train_mask.sum()}\")\n",
    "    print(f\"[INFO] Test samples: {test_mask.sum()}\")\n",
    "    print(f\"[INFO] Device: {device}\")\n",
    "\n",
    "    # Save checkpoint - but device will be re-detected next time\n",
    "    save_checkpoint('training_setup', {\n",
    "        'train_mask': train_mask,\n",
    "        'test_mask': test_mask,\n",
    "        'device': device  # This will be ignored on reload\n",
    "    }, \"Training setup checkpoint saved\")\n",
    "\n",
    "# Initialize model and training components\n",
    "print(f\"[INFO] Initializing model on device: {device}\")\n",
    "model = ImprovedGNN(num_features=data.x.shape[1], hidden_dim=128, dropout=0.3).to(device)\n",
    "\n",
    "# IMPORTANT: Move data to device for NeighborLoader\n",
    "print(\"[INFO] Moving graph data to device...\")\n",
    "data = data.to(device)\n",
    "\n",
    "# Weighted loss for imbalanced classes\n",
    "class_weights_tensor = torch.tensor([class_weight_dict[0], class_weight_dict[1]], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=20)\n",
    "\n",
    "# STEP 7: CREATE NEIGHBORLOADER FOR MEMORY-EFFICIENT TRAINING\n",
    "print(\"[INFO] Creating NeighborLoader for memory-efficient training...\")\n",
    "\n",
    "# Create training and test loaders with neighbor sampling\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[15, 10, 5],  # Sample 15 neighbors for layer 1, 10 for layer 2, 5 for layer 3\n",
    "    batch_size=1024,  # Process 1024 nodes at a time\n",
    "    input_nodes=train_mask,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 for Colab compatibility\n",
    ")\n",
    "\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[15, 10, 5],\n",
    "    batch_size=1024,\n",
    "    input_nodes=test_mask,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Training batches: {len(train_loader)}\")\n",
    "print(f\"[INFO] Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Check for training resume\n",
    "training_checkpoint_path = os.path.join(CHECKPOINT_DIR, 'training_checkpoint.pth')\n",
    "start_epoch = 0\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "test_f1_scores = []\n",
    "\n",
    "if os.path.exists(training_checkpoint_path):\n",
    "    print(\"[RESUME] Loading training checkpoint...\")\n",
    "    checkpoint = torch.load(training_checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    patience_counter = checkpoint['patience_counter']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    test_f1_scores = checkpoint['test_f1_scores']\n",
    "    print(f\"[SUCCESS] Resumed training from epoch {start_epoch}, best F1: {best_f1:.4f}\")\n",
    "\n",
    "print(f\"[INFO] Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"[INFO] Model device: {next(model.parameters()).device}\")\n",
    "print(f\"[INFO] Data device: {data.x.device}\")\n",
    "\n",
    "# STEP 8: Training loop with NeighborLoader (MEMORY EFFICIENT)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ TRAINING WITH NEIGHBORLOADER - MEMORY EFFICIENT ON CUDA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "patience = 50\n",
    "max_epochs = 500\n",
    "\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    # Training with mini-batches\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Batch should already be on the correct device since data is on device\n",
    "        # But let's be explicit\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass on batch\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Evaluation every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.x, batch.edge_index)\n",
    "                pred_proba = F.softmax(out, dim=1)[:, 1]\n",
    "                pred = out.argmax(dim=1)\n",
    "\n",
    "                all_preds.extend(pred.cpu().numpy())\n",
    "                all_labels.extend(batch.y.cpu().numpy())\n",
    "                all_probs.extend(pred_proba.cpu().numpy())\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        y_test_np = np.array(all_labels)\n",
    "        y_pred_np = np.array(all_preds)\n",
    "        y_pred_proba_np = np.array(all_probs)\n",
    "\n",
    "        # Find optimal threshold\n",
    "        if len(np.unique(y_test_np)) > 1:\n",
    "            precision, recall, thresholds = precision_recall_curve(y_test_np, y_pred_proba_np)\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "            best_threshold_idx = np.argmax(f1_scores)\n",
    "            best_threshold = thresholds[best_threshold_idx] if len(thresholds) > 0 else 0.5\n",
    "\n",
    "            y_pred_threshold = (y_pred_proba_np >= best_threshold).astype(int)\n",
    "            test_f1 = f1_score(y_test_np, y_pred_threshold)\n",
    "        else:\n",
    "            test_f1 = 0\n",
    "            best_threshold = 0.5\n",
    "\n",
    "        test_f1_scores.append(test_f1)\n",
    "\n",
    "        # Calculate training accuracy (sample from training loader)\n",
    "        model.eval()\n",
    "        train_preds_agg = []\n",
    "        train_labels_agg = []\n",
    "        train_batches_checked = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in train_loader:\n",
    "                if train_batches_checked >= 5:  # Sample first 5 batches for speed\n",
    "                    break\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch.x, batch.edge_index)\n",
    "                pred = out.argmax(dim=1)\n",
    "                train_preds_agg.extend(pred.cpu().numpy())\n",
    "                train_labels_agg.extend(batch.y.cpu().numpy())\n",
    "                train_batches_checked += 1\n",
    "\n",
    "        train_acc = np.mean(np.array(train_preds_agg) == np.array(train_labels_agg))\n",
    "        test_acc = np.mean(y_pred_np == y_test_np)\n",
    "\n",
    "        print(f'Epoch {epoch:03d}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Threshold: {best_threshold:.3f}')\n",
    "\n",
    "        # Early stopping based on F1 score\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_gnn_model.pth'))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Save training checkpoint every 20 epochs\n",
    "        if epoch % 20 == 0:\n",
    "            save_training_checkpoint(epoch, model, optimizer, scheduler, best_f1, patience_counter, train_losses, test_f1_scores)\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(test_f1)\n",
    "\n",
    "print(f\"\\nâœ… TRAINING COMPLETED ON: {device}\")\n",
    "print(f\"ðŸ“Š Final device check - Model: {next(model.parameters()).device}\")\n",
    "\n",
    "# Final checkpoint save\n",
    "save_training_checkpoint(epoch, model, optimizer, scheduler, best_f1, patience_counter, train_losses, test_f1_scores)\n",
    "\n",
    "# Load best model for final evaluation\n",
    "best_model_path = os.path.join(CHECKPOINT_DIR, 'best_gnn_model.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "# STEP 9: Final evaluation with NeighborLoader (using aggregated results from test loader)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š FINAL EVALUATION WITH NEIGHBORLOADER (Aggregated Test Batches)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        pred_proba = F.softmax(out, dim=1)[:, 1]\n",
    "        pred = out.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(batch.y.cpu().numpy())\n",
    "        all_probs.extend(pred_proba.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_test = np.array(all_labels)\n",
    "y_pred_proba_test = np.array(all_probs)\n",
    "\n",
    "# Find optimal threshold\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_test)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[best_threshold_idx] if len(thresholds) > 0 else 0.5 # Handle case with no positive predictions\n",
    "\n",
    "y_pred_test = (y_pred_proba_test >= optimal_threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "# Note: Accuracy is calculated on aggregated batch predictions, not the true overall test set.\n",
    "test_acc = np.mean(np.array(all_preds) == y_test)\n",
    "# AUC and F1 are correctly calculated on aggregated test predictions and probabilities\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba_test) if len(np.unique(y_test)) > 1 else 0.0 # Handle case with no positive labels in test set\n",
    "final_f1 = f1_score(y_test, y_pred_test) if len(np.unique(y_test)) > 1 else 0.0\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy (Aggregated): {test_acc:.4f}\")\n",
    "print(f\"AUC Score (Aggregated): {auc_score:.4f}\")\n",
    "print(f\"F1 Score (Aggregated): {final_f1:.4f}\")\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Aggregated Test Batches):\")\n",
    "# Ensure target names match unique labels present in y_test\n",
    "target_names = ['Licit', 'Illicit']\n",
    "report = classification_report(y_test, y_pred_test, target_names=target_names, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "\n",
    "print(\"\\nConfusion Matrix (Aggregated Test Batches):\")\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"   Predicted:  Licit  Illicit\")\n",
    "print(f\"   Licit:      {cm[0,0]:5d}     {cm[0,1]:4d}\")\n",
    "print(f\"   Illicit:      {cm[1,0]:3d}     {cm[1,1]:4d}\")\n",
    "\n",
    "\n",
    "# STEP 10: Save the final model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ’¾ SAVING FINAL MODEL WITH NEIGHBORLOADER SUPPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_data = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'addr_to_idx': addr_to_idx,\n",
    "    'feature_cols': feature_cols,\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'class_weights': class_weight_dict,\n",
    "    'model_config': {\n",
    "        'num_features': data.x.shape[1],\n",
    "        'hidden_dim': 128,\n",
    "        'num_classes': 2,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'training_history': {\n",
    "        'train_losses': train_losses,\n",
    "        'test_f1_scores': test_f1_scores,\n",
    "        'final_metrics': {\n",
    "            'test_accuracy': test_acc, # Note: Aggregated accuracy\n",
    "            'auc_score': auc_score,\n",
    "            'f1_score': final_f1,\n",
    "            'optimal_threshold': optimal_threshold\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(model_data, 'improved_gnn_ransomware_model_neighborloader.pth')\n",
    "print(\"[SUCCESS] Final NeighborLoader model saved!\")\n",
    "\n",
    "# Clean up checkpoints (optional)\n",
    "print(\"\\nðŸ§¹ CLEANUP OPTIONS:\")\n",
    "print(\"To clean up checkpoints and save space, run:\")\n",
    "print(\"  import shutil; shutil.rmtree('/content/checkpoints')\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ NEIGHBORLOADER TRAINING ADOPTED:\")\n",
    "print(f\"  âœ… Re-enabled NeighborLoader for memory efficiency\")\n",
    "print(f\"  âœ… Training and evaluation using mini-batches\")\n",
    "print(f\"  âœ… Requires torch-sparse (installed)\")\n",
    "print(f\"  âš ï¸ Evaluation metrics are aggregated over test batches, not the entire test set at once.\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Training Attempted:\")\n",
    "print(f\"  - Training with NeighborLoader should now proceed if torch-sparse is correctly installed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
